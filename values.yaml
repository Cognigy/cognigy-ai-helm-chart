# If cognigy provided mongodb helm chart(https://github.com/Cognigy/cognigy-mongodb-helm-chart) is not in use then set "mongodb.enabled" to false. It will skip doing all corresponding task related to mongodb helm chart, such as creating necessary database. By default it is always enabled.
# mongodb:
#   enabled: false

# This MongoDB user and password should have the permission to create users and databases, so normally it is admin or root
# It does NOT have to be root user. We use these key names to be compatible with Bitnami MongoDB Helm Chert
mongodb:
  enabled: true
  auth:
    rootUser: root
    rootPassword: ""
    ## Existing secret with MongoDB credentials. Mandatory keys: `username` and `password`, that contains the value 
    ## of "rootUser" and "rootPassword"
    ## NOTE: When it's set the previous parameters "rootUser" and "rootPassword" are ignored.
    ##
    existingSecret: ""
  # Default value for the MongoDB replica set deployed by Cognigy MongoDB Helm Chart into 3 availability zones:
  # hosts: mongodb-0.mongodb-headless.mongodb.svc.cluster.local:27017,mongodb-1.mongodb-headless.mongodb.svc.cluster.local:27017,mongodb-2.mongodb-headless.mongodb.svc.cluster.local:27017
  # for development and testing purposes with a single replica MongoDB installation in "mongodb" namespace use:
  # hosts: "mongodb-0.mongodb-headless.mongodb.svc.cluster.local:27017"
  hosts: mongodb-0.mongodb-headless.mongodb.svc.cluster.local:27017,mongodb-1.mongodb-headless.mongodb.svc.cluster.local:27017,mongodb-2.mongodb-headless.mongodb.svc.cluster.local:27017
  ## dbinit generator
  ##
  dbinit:
    image: cognigy.azurecr.io/mongodb:4.4.15-debian-10-r8
    securityContext: {}

## Credentials for pulling image from private image registry.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
## NOTE 1: Either clear text credentials (registry, username and password) or pullSecrets must be provided.
## NOTE 2: If traefik is enabled and you provide clear text credentials, then traefik.deployment.imagePullSecrets must
## be set to "cognigy-registry-token". If you set custom pullSecrets value instead, set the same value under traefik.deployment.imagePullSecrets
imageCredentials:
  ## Alternatively specify the username, password and the url of the private registry.
  ## A kubernetes.io/dockerconfigjson type secret named "cognigy-registry-token" will be created based on these information.
  registry: "cognigy.azurecr.io"
  username: ""
  password: ""

  ## Alternatively specify an array of imagePullSecrets.
  ## Secrets must be manually created in the proper namespace beforehand.
  ## Example:
  ## pullSecrets:
  ##   - cognigyRegistrySecretName
  ##
  ## NOTE: When registry, username and password all are set, the pullSecrets are ignored.
  pullSecrets: []

# Cognigy supports 3 cloud providers:
# - aws
# - azure
# - generic for on-premises installation e.g. with OpenShift
cloud:
  provider: aws
  region: ""

## Cognigy Flow Modules
##
flowModules:
  ## Persistence parameters
  ##
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    ## For AWS cloud provider only
    ##
    aws:
      ## Optionally specify the parameters of an existing Elastic File System or EFS 
      ## ref: https://aws.amazon.com/efs/
      ## Note: By default this is enabled for the AWS cloud provider. When enabled, the value of the efs ID must be provided
      ##
      efs:
        enabled: true
        ## EFS File system ID
        ##
        id: ""
        ## Enable efs csi driver
        ## ref: https://github.com/kubernetes-sigs/aws-efs-csi-driver
        ## Note: By default this is disabled, if enabled then provide the directoryPerms, uid and gid. The default value of directoryPerms, uid and gid are 777, 1000 and 1000 respectively
        efs_csi:
          enabled: false
          directoryPerms: ""
          uid: ""
          gid: ""
    ## Optionally specify StorageClass for flowModules data volume
    ## If defined, storageClassName: <storageClass>
    ## If undefined or set to null, storageClassName is set according to the value defined in "cloud.provider"
    ## 
    storageClass: ""
    ## PVC Storage Request for flowModules data volume.
    ## Default value for aws is 1Mi and for azure 100Gi
    ## Note: The storage request has no effect for NFS
    ##
    size: ""

## Cognigy Functions
##
functions:
  ## Persistence parameters
  ##
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    ## For AWS cloud provider only
    ##
    aws:
      ## Optionally specify the parameters of an existing Elastic File System or EFS 
      ## ref: https://aws.amazon.com/efs/
      ## Note: By default this is enabled for the AWS cloud provider. When enabled, the value of the efs ID must be provided
      ##
      efs:
        enabled: true
        ## EFS File system ID
        ##
        id: ""
        ## Enable efs csi driver
        ## ref: https://github.com/kubernetes-sigs/aws-efs-csi-driver
        ## Note: By default this is disabled, if enabled then provide the directoryPerms, uid and gid. The default value of directoryPerms, uid and gid are 777, 1000 and 1000 respectively
        efs_csi:
          enabled: false
          directoryPerms: ""
          uid: ""
          gid: ""
    ## Optionally specify StorageClass for functions data volume
    ## If defined, storageClassName: <storageClass>
    ## If undefined or set to null, storageClassName is set according to the value defined in "cloud.provider"
    ## 
    storageClass: ""
    ## PVC Storage Request for functions data volume.
    ## Default value for aws is 1Mi and for azure 100Gi
    ## Note: The storage request has no effect for NFS
    ##
    size: ""

## !!! Deprecation Warning !!!
## This "efs" section will be deprecated in the future release. 
## Please use/migrate to "flowModules.persistence.aws.efs.id" and "functions.persistence.aws.efs.id" instead.
##
## For AWS cloud provider only:
##
efs:
  flowModules:
    id: ""
  functions:
    id: ""

cognigyLiveAgent:
  platformToken: ""
  ## Existing secret with live-agent credentials. The secret must have the following key:
  ##   "cognigy-live-agent-platform-token": The token for cognigy live agent
  ##
  ## NOTE: When cognigyLiveAgent.existingSecret is set, clear text token passed in the previous parameter
  ## "cognigyLiveAgent.platformToken" is ignored.
  existingSecret: ""

# Install Management UI on the cluster.
# It is not required, since if not installed and the API endpoint is accessible from the Internet, you can still use the provided Management UI at https://management-ui-v4.cognigy.ai/
managementUi:
  enabled: false
  ingress:
    enabled: true
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  image: cognigy.azurecr.io/management-ui:release-56efad9941-1676559429
  replicaCount: 1
  resources:
    limits:
      memory: "30Mi"
      cpu: "20m"
    requests:
      memory: "10Mi"
      cpu: "10m"
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []

## managementUiCredentials: '[{"username": "example_username", "password": "example_password"}]'
managementUiCredentials: '[]'
## The name of an existing secret with management UI credentials. The secret must have a
## "management-ui-creds.json" key from where the password will be extracted and the content of that key 
## should be in the following format-
##    [{"username": "user", "password": "pass"}, {"username": "user2", "password": "pass2"}]
## NOTE: When this is set, clear text credentials passed in the variable "managementUiCredentials" is ignored.
managementUiCredentialsExistingSecret: ""

## SMTP server information for 'forgot password' functionality.
## Password for the SMTP server.
## A secret named "cognigy-smtp" will be created based on the information provided.
smtpPassword: ""
## The name of an existing secret with SMTP server credentials. The secret must have a
## "system-smtp-password" key from where the password will be extracted.
## NOTE: When this is set, "smtpPassword" is ignored.
smtpPasswordExistingSecret: ""

# This is a temp flag for "Cognigy Apps" which will be removed once we are done with
# fully finishing this feature. Please don't use this flag for now if you are a
# customer running Cognigy.AI/Cognigy Insights using this HelmChart.
cognigyApps:
  enabled: false

## Traefik TLS certificate for the hostname defined at ingress.<service_name>.host
## NOTE: If you provide "tls.enable: true" and "traefik.enabled: true", either tls.crt and tls.key or tls.existingSecret must be provided.
tls:
  ## Enable traefik tls
  ## NOTE: If traefik is enabled ("traefik.enabled: true"), and you provide "tls.enable: false", then the auto redirection of http to https
  ## also must be disabled by setting traefik.ports.web.redirectTo: null
  enabled: true
  ## Add Custom CA certificate. A tls type secret named "cognigy-traefik" will be created based on the values of tls.crt and tls.key
  ## Careful with the indentation
  ## For more information, see https://helm.sh/docs/chart_template_guide/yaml_techniques/#strings-in-yaml
  ##
  ## Custom CA certificate in plaintext, not base64 encoded.
  ## Example:
  ##   crt: |
  ##     -----BEGIN CERTIFICATE-----
  ##     -----END CERTIFICATE-----
  crt: ""
  ## CA certificate private key in plaintext, not base64 encoded.
  ## Example:
  ## key: |
  ##   -----BEGIN PRIVATE KEY-----
  ##   -----END PRIVATE KEY-----
  key: ""
  ## Existing secret with TLS certificates. The secret must have the following two keys:
  ## "tls.crt": Containing the CA certificate
  ## "tls.key": Containing the certificate key
  ## NOTE: When tls.existingSecret is set, clear text certificate passed in the previous parameters "tls.crt" and "tls.key" are ignored.
  existingSecret: ""

# If ingress is not required to deploy then you can set "ingress.enabled" to false. By default it is always enabled.
# ingress:
#   enabled: false
ingress:
  enabled: true
  serviceAnalyticsOdata:
    host: ""
    ## Do not use ingress.<service>.tls.enabled parameter if you have a wildcard certificate for all ingresses (recommended), use `tls` section for a wildcard certificate instead.
    ## If you have individual certificates per ingress, set ingress.<service>.tls.enabled: "true" per individual ingress.
    ## This will create a new secret cognigy-<service-name>-tls containing the individual certificate and the ingress will be served with the individual certificate.
    ## You will need to provide the certificate either in clear text by setting `crt` and `key` fields or as existing base64-encoded kubernetes secret.
    ## For a TLS certificate in clear text set:
    ## "ingress.<service>.tls.crt": TLS certificate including full certificate chain
    ## "ingress.<service>.tls.key": TLS certificate key
    ## NOTE: When ingress.<service>.tls.existingSecret is set, certificate in clear text under `tls.crt` and `tls.key` is ignored.
    ## Please make sure the `existingSecret` is created before referencing it in the ingress specification

    ## CA certificate and certificate key in plaintext, not base64 encoded.
    ## Example:
    ## key: |
    ##   -----BEGIN PRIVATE KEY-----
    ##   -----END PRIVATE KEY-----
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ## Traefik IPWhiteList middleware. IPWhitelist accepts / refuses requests based on the client IP.
    ## A sourceRange of 0.0.0.0/0 means all IPs are allowed. For more detail https://doc.traefik.io/traefik/middlewares/http/ipwhitelist/
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceApi:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceAppSessionManager:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceCollector:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceEndpoint:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceInsightsApi:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceRuntimeFileManager:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceUi:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceStaticFiles:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceWebchat:
    host: ""
    tls:
      enabled: false
      existingSecret: ""
      crt: ""
      key: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0

## Kubernetes service type
##
service:
  serviceAnalyticsOdata:
    ## Optional Service annotations.
    ## Example:
    ## annotations:
    ##   service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
    ##   service.name: service-analytics-odata
    ##
    annotations: {}
  serviceApi:
    annotations: {}
  serviceAppSessionManager:
    annotations: {}
  serviceEndpoint:
    annotations: {}
  serviceInsightsApi:
    annotations: {}
  serviceCollector:
    annotations: {}
  serviceInsightsUi:
    annotations: {}
  serviceRuntimeFileManager:
    annotations: {}
  serviceStaticFiles:
    annotations: {}
  serviceUi:
    annotations: {}
  serviceWebchat:
    annotations: {}
  statefulRabbitMq:
    annotations: {}
  statefulRedis:
    annotations: {}
  statefulRedisPersistent:
    annotations: {}
  serviceNlpEmbeddingEn:
    annotations: {}
  serviceNlpEmbeddingXx:
    annotations: {}
  serviceNlpEmbeddingGe:
    annotations: {}

cognigyEnv:
  NODE_ENV: production

  # Enable the creation of metrics which will then get consumed
  # by our 'service-monitoring'.
  MONITOR_RPC_CALLS: "true"

  # Redis configuration, will soon be a connection string
  REDIS_HOST: redis
  REDIS_PORT: "6379"

  # Redis (persistent) configuration, will soon be a connection string
  REDIS_PERSISTENT_PORT: "6379"
  REDIS_PERSISTENT_HOST: redis-persistent

  # limits (api requests, db queries, context-size)
  MESSAGE_TTL_SECONDS: "120"
  MAX_MEMORY_OBJECT_SIZE: "65536"
  HTTP_JSON_BODY_LIMIT: "65536"
  MAX_BYTE_SIZE: "524288"
  RESPONSE_BYTES_LIMIT: "524288"

  # features (enable / disable)
  FEATURE_CUSTOM_NODES: "true"

  # log cleanup
  LOG_ENTRIES_TTL_IN_MINUTES: "1440"
  LOG_ENTRIES_BUFFER_IN_SECONDS: "5"

  # SMTP server for 'forgot password' functionality
  SYSTEM_SMTP_HOST: "test"
  SYSTEM_SMTP_PORT: "test"
  SYSTEM_SMTP_USERNAME: "test"
  SYSTEM_SMTP_FROM: "test"
  SYSTEM_SMTP_CONNECTION_TYPE: "starttls"
  # SYSTEM_SMTP_PASSWORD is a secret!

  # Domains to whitelist for cors for the API (service-api)
  API_CORS_WHITELIST: ""

  # Execution relevant configuration (service-execution)
  MODULE_MAX_EVENT_EMISSIONS: "10"
  MAX_MODULE_EXECUTION_TIME_IN_SECONDS: "20"

  # Endpoint configuration for Alexa
  ALEXA_END_SESSION_AFTER_EACH_REPLY: "true"

  # Enable max contact profile TTL in minutes
  MAX_CONTACT_PROFILE_TTL_IN_MINUTES: "43200"

  # Enable max conversation TTL in minutes
  MAX_CONVERSATION_TTL_IN_MINUTES: "43200"

  # Enable max conversation TTL in minutes
  MAX_SESSION_STATE_TTL_IN_MINUTES: "10080"

  # Enable new Cognigy Insights UI
  FEATURE_USE_SERVICE_INSIGHTS_UI: "true"

  # Path to files containing pre-computed noise embeddings
  NLP_PRECOMPUTED_EMBEDDINGS_PATH: "/embedding/precomputed_embeddings"
  ### Shared ENV variables ###

  # A link to an online instance of the adaptive cards designer
  # ADAPTIVE_CARDS_DESIGNER_URL:

  # Amazon client credetentials
  # AMAZON_CLIENT_ID:
  # AMAZON_CLIENT_SECRET:

  # Cognigy Live Agent (Chatwoot based)
  # CLIENT_ID_COGNIGY_LIVE_AGENT:

  # Cognigy LiveAgent setup
  # COGNIGY_LIVE_AGENT_API_BASE_URL_WITH_PROTOCOL:
  # COGNIGY_LIVE_AGENT_PLATFORM_TOKEN:

  # URL pointing to the Endpoint service
  # ENDPOINT_BASE_URL_WITH_PROTOCOL:

  # Genesys Cloud handover provider feature flag
  # FEATURE_USE_GENESYS_CLOUD:

  # If set to "true", will exchange "Cogngiy.AI" for a generic name and will let customer to whitelabel the platform
  # FEATURE_USE_WHITELABELING:

  # The JWT secret to sign JWT-tokens with, this is used e.g. for realtime-tokens (endpoint, backend connection)
  # JWT_SECRET:

  # The key for the built-in pendo. If set, pendo routines will be activated.
  # PENDO_KEY:

  # The app-id for the official workplace by facebook cognigy.AI APP
  # WORKPLACE_APP_ID:

  # defaults to 'false', needs to be set to 'true'
  # FEATURE_ALLOW_ADDITIONAL_ACTIONS_IN_CODE_NODES:

  # Disable the license-installation route. This is e.g. used
  # for our cloud installations. The same environment variable
  # needs to be set within the 'service-api' to disable the
  # api there as well. Set the env variable to 'true'
  # DISABLE_LICENSE_UPLOAD:

  # If true, service nlp matcher will be disabled, but not for the ids in FEATURE_USE_SERVICE_NLP_MATCHER_ORG_FILTER
  # FEATURE_DISABLE_SERVICE_NLP_MATCHER:

  # connection details for rabbit mq
  # RABBITMQ_HOST:
  # RABBITMQ_PORT:
  # RABBITMQ_USER:
  # RABBITMQ_PASSWORD:

  # Redis configuration
  # REDIS_HOST:
  # REDIS_PORT:
  # REDIS_USE_TLS:
  # REDIS_KEEP_ALIVE_INTERVAL_IN_SECONDS:

  # Redis persistent configuration
  # REDIS_PERSISTENT_HOST:
  # REDIS_PERSISTENT_PORT:
  # REDIS_PERSISTENT_USE_TLS:
  # REDIS_PERSISTENT_KEEP_ALIVE_INTERVAL_IN_SECONDS:

  # maximum number of unacknowledged messages in Events queues
  # SERVICE_EVENTS_PREFETCH:

  # Can be set to 0 to never expire contact profiles
  # MAX_CONTACT_PROFILE_TTL_IN_MINUTES: "0"

  # Can be set to 0 to never expire session states
  # MAX_SESSION_STATE_TTL_IN_MINUTES: "0"

  ### Platform Integration ENV Variables ###

  # Agent Bot description for Live Agent, Chatwoot, etc. 
  # AGENT_BOT_DESCRIPTION:

  # Agent Bot name for Live Agent, Chatwoot, etc.
  # AGENT_BOT_NAME:

  # Additional Alexa configuration
  # ALEXA_END_SESSION_AFTER_EACH_REPLY:

  # Check avaialability node timeout of agent   
  # CHECK_AGENT_AVAILABILITY_NODE_TIMEOUT_IN_SECONDS:

  # RPC timeout for creating runtime file in seconds. Default value is 8 secs. 
  # CREATE_RUNTIME_FILE_TIMEOUT_IN_SECONDS:

  # Redis default cache TTL
  # DEFAULT_CACHE_TTL:

  # Endpoint base URL 
  # ENDPOINT_BASE_URL_TUNNEL:

  # Allows our customers to specify CORS origins. If you want to specify CORS origins,
  # add all of your origins into a comma-delimited list as the following examples:
  # "http://example1.com, http://example2.com", "http://*.example3.com"
  # ENDPOINT_CORS_WHITELIST:

  # The TTL of the session storage in the endpoint transformers in seconds.
  # ENDPOINT_TRANSFORMER_SESSION_STORAGE_TTL:

  # TTL for user meta-data the system uses to keep track, such as flow changes
  # ENDPOINT_USER_METADATA_REDIS_TTL:

  # Additional facebook configuration. This is to verify the token in the webhook
  # FB_VERIFY_TOKEN:

  # Enable the "Follow Sessions" feature
  # FEATURE_ENABLE_FOLLOW_SESSION:

  # Enable detailed logs for RPC calls handling
  # FEATURE_RPC_LOGS:

  # Enable the SSO features and show the Microsoft nodes if enabled
  # FEATURE_USE_MICROSOFT_SSO:

  # Enable assured message delivery for sockentendpoint
  # FEATURE_USE_SOCKETENDPOINT_EVENTBUFFER:

  # Cleanup/remove idle socket client connection
  # GENESYS_CLOUD_SOCKET_CLIENT_CLEANUP_INTERVAL:

  # To limit the number of unacknowledged messages on a channel/connection on AMQP client 
  # HANDOVER_POLLING_PREFETCH:

  # Salesforce
  # HANDOVER_POLLING_TTL:

  # The TTL for the RPC call to fetch the conversation. 
  # Should be increase if the save interval for analytics is increased
  # HANDOVER_READ_CONVERSATION_TIMEOUT_IN_SECONDS:

  # Handover TTL for persisting agent bot id into redis  
  # HANDOVER_TTL:

  # CORS whitelist for the live-agent API
  # LIVE_AGENT_API_CORS_WHITELIST:

  # Live Agent API secret
  # LIVE_AGENT_API_SECRET:

  # Live Agent base URL
  # LIVE_AGENT_BACKEND_BASE_URL_WITH_PROTOCOL:

  # Message displayed when handover session is aborted 
  # LIVE_AGENT_LITE_EXPIRED_MESSAGE:

  # Time after which handover requests should automatically be closed in minutes. Defaults to 30. 0 means "no cleanup"
  # LIVE_AGENT_LITE_HANDOVER_TTL:

  # The TTL of the JWT + access token in seconds
  # LIVE_AGENT_TOKEN_TTL_IN_SECONDS:

  # Register a rotue for loader.io verification
  # LOADER_IO_TOKEN:

  # The TTL of messages in the endpoint-session-queues in seconds.
  # MESSAGE_TTL_SECONDS:120 

  # Redis key TTL which stores inject/notify meta data 
  # NOTIFY_INJECT_EXPIRATION_IN_SECONDS:

  # To reference assets in the `public` folder
  # PUBLIC_URL:

  # Verify token for RCE 
  # RCE_VERIFY_TOKEN:

  # A comma-separated list of Snapshots that are deployed as Snapshot Executors
  # SNAPSHOTS_USING_SNAPSHOT_EXECUTOR:

  # TTL for the Redis key used to handle client instances 
  # SOCKET_ENDPOINT_REDIS_CLIENT_TTL:

  # TTL for the Redis key used to handle buffer events
  # SOCKET_ENDPOINT_REDIS_CONNECTION_STATUS_TTL:

  # TTL for the Redis key used to handle sockets
  # SOCKET_ENDPOINT_REDIS_SOCKET_TTL:

  # Temporary workaround for a customer:
  # Wait for n seconds before we actually try to read the conversation history
  # as it seems that the history is not complete and
  # that 'waitForPossibleInput' does not work properly
  # TEMP_FIX_WAIT_FOR_CONVERSATION_HISTORY:

  # TWITTER configuration
  # TWITTER_ACCESS_TOKEN:
  # TWITTER_ACCESS_TOKEN_SECRET:
  # TWITTER_APIKEY:
  # TWITTER_CONSUMER_KEY:
  # TWITTER_CONSUMER_SECRET:
  # TWITTER_FLOWNAME:
  # TWITTER_USERNAME:

  # Comma-separated list of origin URIs that are allowed to embed the webchat page as an iframe
  # You can use * in URLs to define subdomains or just * to allow any source
  # Example: WEBCHAT_EMBEDDER_WHITELIST:mywebsite.com,myotherwebsite.com
  # Example: WEBCHAT_EMBEDDER_WHITELIST:*.mywebsite.com
  # Example: WEBCHAT_EMBEDDER_WHITELIST:* 
  # WEBCHAT_EMBEDDER_WHITELIST:

  # Prevent websocket connections from switching to long polling if set to true.
  # WEBCHAT_FORCE_WEBSOCKETS:'false'

  # WhatsApp cloud API url, whereas undefined means default url `https://graph.facebook.com/v13.0`
  # WHATS_APP_GRAPH_API_HOST:

  # Workplace by facebook specific
  # WORKPLACE_VERIFY_TOKEN:
  # WORKPLACE_APP_REDIRECT:
  # WORKPLACE_APP_SECRET:

  ### service-ui env variables ###

  # Enables / Disables some features
  # FEATURE_LICENSE_AGREEMENT:

  # sets the RSS-feed for the projects view
  # RSS_FEED_URL:https://www.cognigy.com/blog/tag/product/rss.xml

  # Flag that can be used to disable the 'get help / get tech support'
  # button within the navigation to file a support ticket a Cognigy
  # DISABLE_GET_TECH_SUPPORT:

  # A link to a custom support-page or solution. Will only be displayed
  # if the 'DISABLE_GET_TECH_SUPPORT' variable is NOT set to 'true'. If
  # defined, it will overwrite our zendesk support integration
  # FEATURE_CUSTOM_SUPPORT_LINK:

  # Flag that can enable the 'community-forum' button within the
  # navigation. The community forum is operated by Cognigy and a platform
  # that allows users to connect with others. By default this will be off.
  # FEATURE_COMMUNITY_BUTTON:

  # Flag used to enable or disable Amazon Lex endpoint
  # FEATURE_ENABLE_AMAZON_LEX_ENDPOINT

  # Allow "from" for smtp configuration, defaults to false
  # FEATURE_USE_SMTP_FROM:

  # Set a Signup Link on the login page
  # FEATURE_USE_SIGNUP_FOR_FREE_LINK:

  ### Temp
  # Activate the translation feature
  # FEATURE_TMP_USE_ENDPOINT_TRANSLATION:

  # Define a whitelist of Endpoints that will not be shown in the UI, seperated by comma.
  # Use the actual endpoint channel type values, e.g. webchat2
  # Example: FEATURE_OMMITED_ENDPOINTS_FROM_UI:"twilio,twilio-autopilot,twilio-sms,webchat2"
  # FEATURE_OMMITED_ENDPOINTS_FROM_UI:

  # Set this to "true" to disable the Insights application and use "the old analytics" page instead
  # This option will be removed in the future!
  # FEATURE_DISABLE_INSIGHTS:

  # Disable marketplace integration
  # FEATURE_DISABLE_MARKETPLACE:

  # Define the marketplace hostname with protocol
  # MARKETPLACE_BASE_URL_WITH_PROTOCOL:

  # Disable the user-journey routes and navigation options in the menu
  # Disable journey-specific RESTful API endpoints in order to avoid that somebody
  # can actually use the journey-system
  # FEATURE_DISABLE_JOURNEYS:

  # Activate possibility of Non-Conversational endpoint creation
  # FEATURE_USE_NON_CONVERSATIONAL_ENDPOINT:

  # whether Cognigy Live Agent should be activated - e.g. has it been deployed?
  # FEATURE_USE_COGNIGY_LIVE_AGENT:

  # If set to "true", the customer will be able to update the 'trustedCode'
  # property of an extension (and its node descriptor set)
  # FEATURE_ALLOW_TRUSTED_CODE_CONFIGURATION:

  # Enable VoiceGateway by setting this to "true"
  # Only works in combination with: FEATURE_ENABLE_VOICEGATEWAY_2_OVERRIDE_ALL_ORG_IDS or FEATURE_ENABLE_VOICEGATEWAY_2_WHITELIST
  # FEATURE_ENABLE_VOICEGATEWAY_2:

  # Enable VoiceGateway to all Org. IDs in the environment by setting it to "true"
  # Only works with FEATURE_ENABLE_VOICEGATEWAY_2 it is also "true"
  # FEATURE_ENABLE_VOICEGATEWAY_2_OVERRIDE_ALL_ORG_IDS:

  # Comma-separated list of Org. IDs to have VoiceGateway enabled
  # Only works with FEATURE_ENABLE_VOICEGATEWAY_2 it is also "true"
  # Example: FEATURE_ENABLE_VOICEGATEWAY_2_WHITELIST:5f99a7ad6107a6be813ff301,5f99a7ad6107a6be813ff302
  # FEATURE_ENABLE_VOICEGATEWAY_2_WHITELIST:

  # Enable Interaction Panel Calls feature to all Org. IDs in the environment by setting it to "true"
  # FEATURE_ENABLE_VOICECALL_OVERRIDE_ALL_ORG_IDS:

  # Comma-separated list of Org. IDs to have Interaction Panel Calls feature enabled
  # Example: FEATURE_ENABLE_VOICECALL_WHITELIST:5f99a7ad6107a6be813ff301,5f99a7ad6107a6be813ff302
  # FEATURE_ENABLE_VOICECALL_WHITELIST:

  # Enable Central Configuration for Adaptive cards
  # FEATURE_USE_ADVANCED_ADAPTIVECARDS_INTEGRATION:

  # Enable this flag to use the new Insights UI service
  # FEATURE_USE_SERVICE_INSIGHTS_UI:

  # Enables training all out of date flow models from a project - needs it's own nlp deployment.
  # FEATURE_TRAIN_ALL_PROJECT_FLOWS:

  # Route to the Insights UI service for local development
  # INSIGHTS_UI_BASE_URL_WITH_PROTOCOL:

  # Temporary flag to hide Yes/No Intent feature.
  # Will be removed when feture is complete..
  # FEATURE_ENABLE_YES_NO_INTENTS:

  # The websocket URI to reach Voice Gateway from Interaction Panel Test calls
  # VOICE_GATEWAY_SIP_WS_URI_WITH_PROTOCOL:

  ### service-resources env variables ###

  # database credentials for resources
  # RESOURCES_DB_USERNAME:
  # RESOURCES_DB_PASSWORD:
  # RESOURCES_DB_HOST:
  # RESOURCES_DB_PORT:
  # RESOURCES_DB_NAME:

  # database credentials for nlu grid fs bucket
  # NLU_DB_USERNAME:
  # NLU_DB_PASSWORD:
  # NLU_DB_HOST:
  # NLU_DB_PORT:
  # NLU_DB_NAME:

  # encryption key for fields within connections
  # CONNECTION_FIELDS_ENCRYPTION_KEY:

  # Max amount of undo / redo steps to store
  # MAX_UNDO_REDO_STEPS:

  # Max amount of locales per project. Defaults to 10
  # MAX_AMOUNT_OF_PROJECT_LOCALES:

  # The storage path where Cognigy functions will reside
  # FUNCTION_CODE_STORAGE_PATH:

  # The amount of seconds a downloadable package for a Snapshot exists in Redis
  # SNAPSHOT_PACKAGE_TTL_IN_SECONDS:

  # list of organisationIds that use service nlp matcher, seperated by ;
  # FEATURE_USE_SERVICE_NLP_MATCHER_ORG_FILTER:

  # The Path from which system wide extensions should be loaded.
  # FEATURE_ADDITIONAL_SYSTEM_WIDE_EXTENSIONS_PATH:

  # The total number of http requests a customer can send via the httpRequest API
  # OPTIONS_RESOLVER_MAX_OUTGOING_HTTP_REQUESTS:

  # The total amount of data (in bytes) a http response can contain - used for the
  # httpRequest API
  # OPTIONS_RESOLVER_MAX_HTTP_REQUEST_RESPONSE_SIZE_BYTES:

  # The maximum number of seconds an options resolver is allowed to run, after 
  # which the vm running the resolver will exit
  # OPTIONS_RESOLVER_MAX_EXECUTION_TIME_IN_SECONDS:

  # Number of minutes after which playbook runs expire - default
  # is set to 30 days
  # FUNCTION_RUN_EXPIRATION_IN_MINUTES:

  # "true" if extract answer feature should be made available
  # FEATURE_ENABLE_EXTRACT_ANSWER:

  # Generative AI
  # GENERATIVE_AI_GENERATE_SENTENCES_PROMPT:
  # GENERATIVE_AI_GENERATE_SENTENCES_TIMEOUT_MS:
  # GENERATIVE_AI_LEXICON_ENTRIES_PROMPT:
  # GENERATIVE_AI_LEXICON_ENTRIES_TIMEOUT_MS:
  # GENERATIVE_AI_FLOW_GENERATION_DESCRIPTION_PROMPT:
  # GENERATIVE_AI_FLOW_GENERATION_TRANSCRIPT_PROMPT:
  # GENERATIVE_AI_FLOW_GENERATION_DESCRIPTION_TIMEOUT_MS:

  ### service-security env variables ###

  # connection details for the database
  # SECURITY_DB_USERNAME:
  # SECURITY_DB_PASSWORD:
  # SECURITY_DB_HOST:
  # SECURITY_DB_PORT:
  # SECURITY_DB_NAME:

  # The TTL of the LRU cache that stores the billing timezone
  # LRU_CACHE_BILLING_TIMEZONE_TTL_IN_SECONDS:

  # The prefetch for the conversation counter queue. Default 50
  # CONVERSATION_COUNTER_PREFECTH:

  ### service-api env variables ###

  # licensing server url, default value present
  # LICENSING_SERVER_URL_WITH_PROTOCOL:

  # cognigy licensing server, default value present
  # LICENSING_PING_INTERVAL_IN_MINUTES:

  # ability to disable reporting (reporting is enabled by default!)
  # LICENSING_DISABLE_REPORTING:

  # connection details for the authentication database
  # AUTHENTICATION_DB_USERNAME:
  # AUTHENTICATION_DB_PASSWORD:
  # AUTHENTICATION_DB_HOST:
  # AUTHENTICATION_DB_PORT:
  # AUTHENTICATION_DB_NAME:

  # Redis persistent configuration
  # REDIS_PERSISTENT_PASSWORD:

  # connection details for smtp
  # SYSTEM_SMTP_HOST:
  # SYSTEM_SMTP_PORT:
  # SYSTEM_SMTP_CONNECTION_TYPE:
  # SYSTEM_SMTP_USERNAME:
  # SYSTEM_SMTP_PASSWORD:
  # SYSTEM_SMTP_FROM:

  # Session Secret is used e.g. for the openid-connect implementation
  # SESSION_SECRET:

  # The basic auth credentials for special api calls (like creating organisations)
  # INTERNAL_API_USERNAME:
  # INTERNAL_API_PASSWORD:

  # Static URLs to parts of the system 
  # protocol + domain + port, without trailing slash
  # e.g. "https://subdomain.domain.tld:1234"
  # FRONTEND_BASE_URL_WITH_PROTOCOL:
  # BACKEND_BASE_URL_WITH_PROTOCOL:
  # WEBCHAT_BASE_URL_WITH_PROTOCOL:

  # Decides whether users need to accept a license agreement 
  # "true" | "false" (to be extended to "false" | "demo" | "community" etc. for different agreement texts)
  # FEATURE_LICENSE_AGREEMENT:
  # FEATURE_APIKEY_AUTH: 

  # limit in kb for http put and post
  # HTTP_JSON_BODY_LIMIT:60

  # webhook that gets executed when a new user was created using the internal API
  # WEBHOOK_ON_USER_CREATED_HOST:

  # limit for http bodyparser text, used in upload csv
  # example: '1000KB', '2MB', ... 
  # HTTP_TEXT_BODY_LIMIT:

  # Allows our customers to specify CORS origins. If you want to specify CORS origins,
  # add all of your origins into a comma-delimited list like so:
  # "http://example1.com, http://example2.com"
  # API_CORS_WHITELIST:

  # Allows to defined the TTL of access-tokens in minutes, e.g. set it to "5"
  #
  # The default value is: 15 minutes.
  # API_ACCESS_TOKEN_EXPIRATION_IN_MINUTES:

  # Allows to define the TTL of refresh-tokens. They need to have at least 4
  # times the amount of TTL than access-tokens!. E.g. set this to "15".
  #
  # The default value is: 1 day.
  # API_REFRESH_TOKEN_SHORT_EXPIRATION_IN_MINUTES:

  # The default value is: 30 days.
  # API_REFRESH_TOKEN_LONG_EXPIRATION_IN_MINUTES:

  # Can be set to 0 to never expire contact profiles
  # MAX_CONVERSATION_TTL_IN_MINUTES:

  # The amount of seconds how long audit-events are buffered before
  # they will get written to the databaes
  # AUDIT_EVENT_BUFFER_TIME_IN_SECONDS:

  # The amount of minutes for how long an audit-event should exist within the
  # system. By default, our system will drop audit-events after 30 days.
  # AUDIT_EVENT_TTL_IN_MINUTES:

  # The amount of operations per batch call.
  # MAX_AMOUNT_OF_OPERATIONS_PER_BATCH_CALL:

  # The max size allowed when uploading a Snapshot file.
  # Default size is 256 MiB
  # SNAPSHOT_MAX_FILE_SIZE:

  # The max size allowed when uploading a Package file.
  # Default size is 256 MiB
  # PACKAGE_MAX_FILE_SIZE:

  # Enable compression of response objects. Is by default
  # enabled and has to be set explicitly to 'false' to be disabled.
  # API_ENABLE_COMPRESSION:

  # The max size allowed when uploading a extensions file.
  # Default size is 128 MiB
  # EXTENSION_MAX_FILE_SIZE:

  # The length of the access token we generate. Default is 512 bytes
  # SECURITY_ACCESS_TOKEN_LENGTH:

  # The length of the refresh token we generate. Default is 2048 bytes
  # SECURITY_REFRESH_TOKEN_LENGTH:

  # Enable the feature to create superapikeys
  # FEATURE_USE_SUPERAPIKEY_API:

  # The allowed hostnames, from where extensions can be remotely loaded.
  #
  # This should be a comma-separated list of hostnames, domain names, or a mixture
  # of both. Asterisks can be used as wildcards. 
  # Domain names may be indicated by a leading dot.
  # example: "*.aventail.com,home.com,.seanet.com"
  # defaults to "" meaning, no host is allowed.
  # use "*" to allow all hostnames.
  # ALLOWED_EXTENSION_HOSTNAMES:

  # If set to an integer value, the amount of Refresh Tokens generated by a user
  # will be limited to this value.
  # REFRESH_TOKEN_MAX_AMOUNT_PER_USER:

  # location of the authentication json for using the management ui, default: /run/secrets/management-ui-creds.json
  # AUTHENTICATION_MANAGEMENT_UI_CRED_LOCATION:

  #
  # Cognigy Live Agent (chatwoot based)
  #
  # CLIENT_SECRET_COGNIGY_LIVE_AGENT:
  # REDIRECT_URI_COGNIGY_LIVE_AGENT:

  # Email used to validate the organisation delition process
  # SYS_ADMINISTRATOR_EMAIL:

  # DELETE_ORGANIZATION_TOKEN_TTL_IN_MINUTES:

  # MANAGEMENTUI_BASE_URL_WITH_PROTOCOL:

  # Flag to conditionally use request-promise to execute transformer
  # USE_REQUEST_PROMISE_FOR_TRANSFORMER:

  # VG prepareCall api-key
  # VOICE_TEST_CALL_API_SECRET:
  # VG base url
  # VOICE_GATEWAY_BASE_URL_WITH_PROTOCOL:
  # VOICE_GATEWAY_PREPARE_CALL_API:

  # The timeout for getting the conversation counter.
  # default 60000
  # CONVERSATION_COUNTER_TIMEOUT_IN_MS:

  ### service-journeys env variables ###

  # database credentials for resources
  # JOURNEYS_DB_USERNAME:
  # JOURNEYS_DB_PASSWORD:
  # JOURNEYS_DB_HOST:
  # JOURNEYS_DB_PORT:
  # JOURNEYS_DB_NAME:

  # Hubspot Middleware URL to track events
  # HSMWURL:

  # URL to Microsoft Flow for Error Alerting
  # HSMWALERTURL:

  ### service-trainer env variables ###

  # connection details for the database
  # TRAINER_DB_USERNAME:
  # TRAINER_DB_PASSWORD:
  # TRAINER_DB_HOST:
  # TRAINER_DB_PORT:
  # TRAINER_DB_NAME:

  # An aes encryption key
  # TRAINER_PACKAGE_ENCRYPTION_KEY:

  ### service-logs env variables ###

  # connection details for the database
  # LOG_DB_USERNAME:
  # LOG_DB_PASSWORD:
  # LOG_DB_HOST:
  # LOG_DB_PORT:
  # LOG_DB_NAME:

  # ttl & buffer-time for log entries
  # LOG_ENTRIES_TTL_IN_MINUTES:
  # LOG_ENTRIES_BUFFER_IN_SECONDS:

  # maximum entries we process at once
  # LOG_ENTRIES_MAX_BUFFER_LENGHT:

  # the desired maximum length of the queue
  # MAX_LOGS_AMOUNT_IN_REDIS:

  ### service-task-manager env variables ###

  # connection details for the database
  # TASK_MANAGER_DB_USERNAME:
  # TASK_MANAGER_DB_PASSWORD:
  # TASK_MANAGER_DB_HOST:
  # TASK_MANAGER_DB_PORT:
  # TASK_MANAGER_DB_NAME:

  # set the ttl, how long a task is saved in the database
  # default is 1 week
  # TASK_TTL_IN_MINUTES:

  # Train intents expiration timeout in milliseconds. Defaults to "60000".
  # TRAIN_INTENTS_EXPIRATION_MILLISECONDS:

  ### service-custom-modules env variables ###

  # Whether to use a cert for installing npm modules
  # USE_PROXY_CA_CERT:

  ### service-ai env variables ###

  # max number of queues for consistent-hash-exchange in service-ai. default is 20.
  # AI_MESSAGE_QUEUES_AMOUNT: "20"

  # queue TTL in seconds Queues will expire after a period of time
  # only when they are not used (e.g. do not have consumers). default is 30 seconds.
  # AI_MESSAGE_QUEUES_TTL_IN_SECONDS: "30"

  # maximum byte size for context, contact profiles, input data
  # MAX_MEMORY_OBJECT_SIZE: ""

  # maximum loops to allow in Flows
  # MAX_LOOPS: "4"

  # Use NLU 2.0 for short utterances
  # SHORT_UTTERANCES_V2: ""

  # The TTL of the session state in redis. Default is 10 minutes
  # SESSION_STATE_REDIS_TTL_IN_SECONDS: "600"

  # The TTL of the session storage in the NLU transformers in seconds.
  # NLU_TRANSFORMER_SESSION_STORAGE_TTL: ""

  # The max outgoing hhtp requests that can be executed in a NLU transformer
  # NLU_TRANSFORMER_MAX_OUTGOING_REQUESTS: ""

  # The amount of times we will retry sending an email
  # SMTP_RETRY_ATTEMPTS: ""

  # The maximum attachment size allowed when sending an email
  # SMTP_MAX_ATTACHMENT_SIZE: ""

  # The amount of entries to be buffered, till the batch get sent
  # BULK_CREATE_TRAINER_MAX_BATCH_SIZE: ""

  # Timeout for 'getNluResultsRpc' call
  # RPC_TIMEOUT_GET_NLU_RESULTS_IN_MS: "5000"

  # The path to the HTMLTemplate for the email notification Node.
  # if not provided the value defaults to cognigy internal email html template
  # EMAIL_NOTIFICATION_HTML_TEMPLATE_FILE_PATH: ""

  # Maximum size for Fuzzy Search Node source data (in bytes)
  # FUZZYSEARCH_MAX_OBJECT_SIZE: ""

  # The amount of messages one AI should handle in parallel
  # AI_MESSAGE_PREFETCH_COUNT: ""

  # Whether to use the new queueing concept that better handles messages in parallel
  # FEATURE_USE_QUEUEING_V2: ""

  # Controls the LRU cache for the chart executable
  # AI_LRU_CACHE_CHART_EXECUTABLE_ENABLED: "true"
  # AI_LRU_CACHE_CHART_EXECUTABLE_MAX_SIZE: "1000"
  # AI_LRU_CACHE_CHART_EXECUTABLE_MAX_AGE_IN_SECONDS: "86400"

  # Controls the LRU cache for the project data
  # AI_LRU_CACHE_PROJECT_ENABLED: "true"
  # AI_LRU_CACHE_PROJECT_MAX_SIZE: "1000"
  # AI_LRU_CACHE_PROJECT_MAX_AGE_IN_SECONDS: "86400"

  # Controls the LRU cache for connections
  # AI_LRU_CACHE_CONNECTIONS_MAX_AGE_IN_SECONDS: "86400"
  # AI_LRU_CACHE_CONNECTIONS_ENABLED: "true"
  # AI_LRU_CACHE_CONNECTIONS_MAX_SIZE: "1000"

  # Whether to refresh the Profile on each input
  # AI_REFRESH_PROFILES_ENABLED: ""

  # Max timeout fot the loadSessionStateRpc call
  # AI_LOAD_SESSION_STATE_RPC_TIMEOUT_IN_SECONDS: "2"

  # Timeout for HTTP requests for the httpRequest Node
  # HTTP_NODE_TIMEOUT_IN_SECONDS: ""

  # The TTL of Brains in AI. Default is 10 minutes
  # AI_BRAIN_TTL: ""

  # The cleanup interval of Brains in AI. Default is 30s
  # AI_BRAIN_CLEANUP_INTERVAL: ""

  # Maximum count of flows that we keep in memory cache. Default 1000
  # AI_FLOW_CACHE_MAX_SIZE=
  # How many seconds we keep a flow in memory cache without being called. Default 3600 
  # AI_FLOW_CACHE_TTL_IN_SEC=

  # The cleanup interval of session states in seconds. Default is 30
  # AI_SESSION_STATE_CACHE_CLEANUP_INTERVAL_IN_SEC=
  # How many seconds we keep a session state in memory. Default is 3600 
  # AI_SESSION_STATE_CACHE_TTL_IN_SEC=
  # Maximum items that are handled in one cleanup cycle for the session state. Default is 100.
  # AI_SESSION_STATE_CACHE_MAX_COUNT_PER_CLEANUP_CYCLE=

  # Timeout for Question node datepicker function.
  # AI_DATEPICKER_FUNCTION_VM_TIMEOUT_IN_MS: "150"

  # Generative AI
  # FEATURE_TMP_GENERATIVE_AI_REPHRASE_STATMENT_PROMPT:
  # FEATURE_TMP_GENERATIVE_AI_REPHRASE_MULTIPLE_STATMENTs_PROMPT:
  # FEATURE_GENERATIVE_AI_REPHRASE_TIMEOUT_IN_SECONDS:

  ### service-execution env variables ###

  # Default value is 512 MiB
  # MAX_EXTENSIONS_CACHE_DIR_SIZE_IN_MB: "512"

  # Path to extensions cache directory
  # PATH_TO_EXTENSIONS_CACHE_DIR: ""

  # Amount of extensions that will be dropped from extensions map if the max dir size exceeds
  # AMOUNT_TO_DROP_IF_MAX_EXTENSIONS_DIR_SIZE_EXCEEDS: ""

  # Whether Extensions should be executed in a VM. Default is true
  # EXECUTE_EXTENSIONS_IN_VM: "true"

  ### service-function-execution env variables ###

  # The total number of http requests a customer can send via the httpRequest API
  # FUNCTION_MAX_OUTGOING_HTTP_REQUESTS: ""

  # The total amount of data (in bytes) a http response can contain - used for the
  # httpRequest API
  # FUNCTION_MAX_HTTP_REQUEST_RESPONSE_SIZE_BYTES: ""

  # The maximum amount of minutes a function instance is allowed to run
  # FUNCTION_EXECUTION_MAX_EXECUTION_TIME_IN_MINUTES: ""

  ### service-function-scheduler env variables ###

  # how long should we store instance information in the database -
  # we use a time-based index in order to rotate the data
  # FUNCTION_INSTANCE_EXPIRATION_IN_MINUTES: ""

  # the maximum number of active/running function instances per
  # organisation
  # FUNCTION_INSTANCE_MAX_RUNNING_NUMBER_PER_ORGANIZATION: ""

  # the maximum size (in bytes) for the parameters object with which
  # a function instance can be started, default 128 KB
  # FUNCTION_PARAMETERS_SIZE_MAX_IN_BYTES: ""

  ### service-http env variables ###

  # The amount of messages service-http can handle in parallel
  # SERVICE_HTTP_PREFETCH: ""

  # The max limit of a response object in bytes
  # RESPONSE_BYTES_LIMIT: ""

  ### service-parser env variables ###

  # The timeout for rpc calls made by service-parser
  # SERVICE_PARSER_RPC_TIMEOUT_IN_SEC: "20"

  # The chunk size for intent sentences to send to service-resources
  # SERVICE_PARSER_INTENT_SENTENCE_CHUNKSIZE: "300"

  # The chunk size for lexicon entries to send to service-resources
  # SERVICE_PARSER_LEXICON_ENTRY_CHUNKSIZE: "300"

  # The max amount of lexicon entries service-parser can import
  # SERVICE_PARSER_MAXIMUM_LEXICONS_ENTRIES_IMPORT: "1000000"

  ### service-playbook-execution env variables ###

  # Maximum excutions Default:10
  # MAX_CONCURRENT_PLAYBOOK_EXECUTIONS: "10"

  ### service-profiles env variables ###

  # connection details for the database
  # PROFILES_DB_USERNAME:
  # PROFILES_DB_PASSWORD:
  # PROFILES_DB_HOST:
  # PROFILES_DB_PORT:
  # PROFILES_DB_NAME:

  ### service-runtime-file-manager env variables ###

  # Allows our customers to specify CORS origins. If you want to specify CORS origins,
  # add all of your origins into a comma-delimited list like so:
  # "http://example1.com, http://example2.com"
  # RUNTIME_FILE_MANAGER_CORS_WHITELIST: ""

  # Max file size of a runtime file in bytes. default :1024*1024*10 (10MB) 
  # RUNTIME_FILE_MANAGER_MAX_FILE_SIZE: "10485760"

  ### service-session-state-manager env variables ###

  # The amount of time to buffer events before saving them. Can be set to 0 to disable buffering
  # SESSION_STATE_MANAGER_BUFFER_TIME_IN_SECONDS: "10"

  # The amount of events to buffer before saving them. Can be set to 0 to disable buffering
  # SESSION_STATE_MANAGER_BUFFER_COUNT: 100

  ### service-nlp env variables ###

  # Optional log level string: INFO | DEBUG | ERROR
  # NLP_LOG_LEVEL: ""

  # The amqp client prefetch count for parallel processing of TRAIN messages.
  # NLP_PREFETCH_COUNT: "1"

  # The amqp client prefetch count for parallel processing of SCORE messages
  # NLP_SCORE_PREFETCH_COUNT: "5"

  # The maximum number of classes considered for deep training. Defaults to 2000.
  # NLP_MAX_STATE_CONDITION_TRAINING_CLASSES: ""

  # The maximum number of classes considered for deep training. Defaults to 100.
  # NLP_STATE_CONDITION_MASKED_CLASSIFICATION_THRESHOLD: ""

  # The maximum length of inputs/example sentences for exact matching. Defaults to 2048.
  # NLP_MAX_MATCHER_LEN: ""

  # Whether to use featurization script or not. Defaults to False.
  # NLP_ENCODER_SCRIPT: ""

  # Score cache TTL in seconds. Defaults to 600.
  # NLP_CACHE_TTL: ""

  # Score ttl cache size. Defaults to 128.
  # NLP_CACHE_SIZE: ""

  # NLP_SCORE_DISABLE_CACHE_EVICTION:"False"

  # Maximum size of a 'small' training batch for direct featurization. Defaults to 20.
  # NLP_SMALL_TRAINING_BATCH_SIZE: ""

  # Use the legacy ttl cache for intent train groups. New caching as opt-out. Defaults to False
  # NLP_SCORE_USE_PRIORITY_EVICTION_CACHE: ""

  # The percentage of available memory the traingroup cache can use. Defaults to 75.
  # NLP_SCORE_CACHE_PERCENTAGE_OF_AVAILABLE_MEMORY: ""

  # Interval for recalculating eviction priority and removal of unused data. Defaults to 60
  # NLP_SCORE_CACHE_UPDATE_INTERVAL_IN_MINUTES: ""

  # How many interval timeframes are used for recalculating eviction priority and removal of unused data. Defaults to 24
  # NLP_SCORE_CACHE_AMOUNT_OF_TIMEFRAMES: ""

  # Log memory consumption on every hourly update. Defaults to False
  # NLP_SCORE_LOG_CACHE_MEM: ""

  # Not case sensitive.
  # "All" when all possible Any Slot matches should be returned. 
  # "Exact" when only the exact Any Slot match should be returned, i.e. the exact sentence 
  # structure with the Any Slot.
  # Defaults to "Default".
  # Default behaviour of Any Slot matching is to return the last found match, regardless of the exact sentence structure.
  # NLP_ANYSLOT_RETURN_MODE: "Default"

  # "True" to enable the logger to print timing information in some log messages, defaults to "False"
  # NLP_ENABLE_PERFORMANCE_LOGS: "True"

  # "True" to print debug log messages as info log messages. Defaults to "False"
  # NLP_DEBUG_IS_INFO: "False"

  ### service-nlp-matcher env variables ###

  # amount of items we store in the memory cache, default 20000
  # NLP_MATCHER_CACHE_ITEMS: ""

  # batch size for get keyphrase tokens
  # GET_TOKENS_BATCH_SIZE: ""

  # log level
  # NLP_MATCHER_LOG_LEVEL: ""

  # Prefetch & TTL on the read and write queues.
  # When you change this in a rolling update, you will have to scale down service matcher first, so that the existing queues are gone.
  # You cannot change queues properties of existing queues.
  # NLP_MATCHER_PREFETCH_COUNT_READ: ""
  # NLP_MATCHER_PREFETCH_COUNT_WRITE: ""
  # NLP_MATCHER_QUEUE_TTL_IN_SECS_READ: ""
  # NLP_MATCHER_QUEUE_TTL_IN_SECS_WRITE: ""

  ### service-nlp-ner env variables ###

  # Maximum restarts of child process before process exit
  # NER_MAX_CHILD_PROCESS_RESTARTS: ""

  # Timeout in ms before restarting the child process on error
  # NER_CHILD_PROCESS_RESTART_TIMEOUT: ""

  # Timeout in ms for getting the results from duckling & rustling
  # NER_CHILD_PROCESS_REPLY_TIMEOUT: ""

  # rabbitmq prefetch for getNer call
  # GET_NER_PREFETCH: ""

  # disable starting rustling & duckling http severs as child processes
  # NLP_NER_DISABLE_CHILD_PROCESSES: ""

  ### service-analytics-reporter env variables ###

  ## connection details for the database
  # ANALYTICS_REPORTER_DB_USERNAME:
  # ANALYTICS_REPORTER_DB_PASSWORD:
  # ANALYTICS_REPORTER_DB_HOST:
  # ANALYTICS_REPORTER_DB_PORT:
  # ANALYTICS_REPORTER_DB_NAME:

  # # connection details for rabbit mq
  # RABBITMQ_HOST:
  # RABBITMQ_PORT:
  # RABBITMQ_USER:
  # RABBITMQ_PASSWORD:

  # # connection details for redis
  # REDIS_HOST:
  # REDIS_PORT:
  # REDIS_PASSWORD:

  # # how long should Cognigy Insights cache generated reports until
  # # they have to be re-generated? (default 180 minutes)
  # INSIGHTS_REPORTS_CACHE_TIME_IN_MINUTES:

  ### service-analytics-collector env variables ###
  # # connection details for the database
  # ANALYTICS_COLLECTOR_DB_USERNAME:
  # ANALYTICS_COLLECTOR_DB_PASSWORD:
  # ANALYTICS_COLLECTOR_DB_HOST:
  # ANALYTICS_COLLECTOR_DB_PORT:
  # ANALYTICS_COLLECTOR_DB_NAME:

  # # connection details for rabbit mq
  # RABBITMQ_HOST:
  # RABBITMQ_PORT:
  # RABBITMQ_USER:
  # RABBITMQ_PASSWORD:

  # # connection details for redis
  # REDIS_HOST:
  # REDIS_PORT:
  # REDIS_USE_TLS:
  # REDIS_KEEP_ALIVE_INTERVAL_IN_SECONDS:

  ### service-analytics-conversations env variables ###
  # # connection details for the database# connection details for the database
  # CONVERSATION_COLLECTOR_DB_USERNAME:
  # CONVERSATION_COLLECTOR_DB_PASSWORD:
  # CONVERSATION_COLLECTOR_DB_HOST:
  # CONVERSATION_COLLECTOR_DB_PORT:
  # CONVERSATION_COLLECTOR_DB_NAME:

  # # connection details for rabbit mq
  # RABBITMQ_HOST:
  # RABBITMQ_PORT:
  # RABBITMQ_USER:
  # RABBITMQ_PASSWORD:

  # # connection details for redis
  # REDIS_HOST:
  # REDIS_PORT:
  # REDIS_USE_TLS:
  # REDIS_DEFAULT_TTL:

  # enabled detailed logs for RPC calls handling
  # FEATURE_RPC_LOGS:

  # # how long should Cognigy Insights cache generated reports until
  # # they have to be re-generated? (default 180 minutes)
  # INSIGHTS_REPORTS_CACHE_TIME_IN_MINUTES:

  # Conversation collection data Expiration time in minutes
  # MAX_CONVERSATION_TTL_IN_MINUTES:

  # contact Profile Expiration in minutes
  # MAX_CONTACT_PROFILE_TTL_IN_MINUTES:


  ### service-analytics-odata env variables ###

  # # connection details for the odata database with analytics collection
  # ANALYTICS_COLLECTOR_DB_USERNAME:
  # ANALYTICS_COLLECTOR_DB_PASSWORD:
  # ANALYTICS_COLLECTOR_DB_HOST:
  # ANALYTICS_COLLECTOR_DB_PORT:
  # ANALYTICS_COLLECTOR_DB_NAME:

  # # connection details for the database with organisation collection
  # SECURITY_DB_USERNAME:
  # SECURITY_DB_PASSWORD:
  # SECURITY_DB_HOST:
  # SECURITY_DB_PORT:
  # SECURITY_DB_NAME:

  # # connection details for the flows database
  # FLOWS_DB_USERNAME:
  # FLOWS_DB_PASSWORD:
  # FLOWS_DB_HOST:
  # FLOWS_DB_PORT:
  # FLOWS_DB_NAME:

  # # odata server port
  # ODATA_HOSTNAME:

  # # Should be either http or https
  # ODATA_PROTOCOL:

  # # The amount of seconds a query
  # # is cached in Redis
  # ODATA_CACHE_TIME_IN_SECONDS:

  # # connection details of our rabbit mq installation
  # RABBITMQ_HOST:
  # RABBITMQ_PORT:
  # RABBITMQ_USER:
  # RABBITMQ_PASSWORD:

  # # connection details for redis
  # REDIS_HOST:
  # REDIS_PORT:
  # REDIS_USE_TLS:
  # REDIS_KEEP_ALIVE_INTERVAL_IN_SECONDS:

  ### service-insights-api env variables ##
  # RABBITMQ_HOST:
  # RABBITMQ_PORT:
  # RABBITMQ_USER:
  # RABBITMQ_PASSWORD:

  # REDIS_HOST:
  # REDIS_PORT:
  # REDIS_PASSWORD:


  # INSIGHTS_JWT_SECRET:

  # INSIGHTS_BACKEND_BASE_URL_WITH_PROTOCOL:

  # # If set to "true", will exchange "Cogngiy.AI" for a generic name
  # FEATURE_USE_WHITELABELING:

  # INSIGHTS_API_CORS_WHITELIST:

  # HTTP_JSON_BODY_LIMIT:

  # PRODUCT_DISPLAY_NAME:

  # FEATURE_RPC_LOGS:

  # SERVICE_PREFIX:
  # INSIGHTS_API_VERSION:
  # PRODUCT_DISPLAY_NAME:

  ### service-insights-api env variables ##
  # urls where services are reachable

  # FRONTEND_BASE_URL_WITH_PROTOCOL:
  # BACKEND_BASE_URL_WITH_PROTOCOL:
  # ENDPOINT_BASE_URL_WITH_PROTOCOL:
  # WEBCHAT_BASE_URL_WITH_PROTOCOL:
  # INSIGHTS_BACKEND_BASE_URL_WITH_PROTOCOL:

  # # The key for the built-in pendo. If set, pendo routines will be activated.

  # PENDO_KEY:

  # # Flag that can be used to disable the 'get help / get tech support'

  # # button within the navigation to file a support ticket a Cognigy

  # DISABLE_GET_TECH_SUPPORT:true

  # # Allow the customer to whitelabel our frontend

  # FEATURE_USE_WHITELABELING:false

  # # Cognigy Live Agent (chatwoot based)

  # CLIENT_ID_COGNIGY_LIVE_AGENT:

  # # whether Cognigy Live Agent should be activated - e.g. has it been deployed?

  # FEATURE_USE_COGNIGY_LIVE_AGENT:
  # FEATURE_USE_COGNIGY_DOC_LINKS:

  # whether Cognigy Live Agent Dashboard should be activated
  # FEATURE_USE_COGNIGY_LIVE_AGENT_DASHBOARD:

  ### service-collector env variables ###
  # RabbitMQ configuration

  # RABBITMQ_HOST:
  # RABBITMQ_PORT:
  # RABBITMQ_USER:
  # RABBITMQ_PASSWORD:

  # # api key configuration
  # INSIGHTS_COLLECTOR_API_KEY:

  # # http2 configuration (optional)(defaults to 0.0.0.0:8000) 
  # INSIGHTS_COLLECTOR_HTTP_HOST:
  # INSIGHTS_COLLECTOR_HTTP_PORT:

  ### management-ui env variables ###

  # The port of the Cognigy Management UI
  # CMUI_HTTP_PORT:

amazonCredentials:
  ## The client id from amazon.developers.com
  clientId: ""
  ## The client secret from amazon.developers.com
  clientSecret: ""
  ## Existing secret with amazon credentials. The secret must have the following two keys:
  ##   "amazon-client-id": The client id from amazon.developers.com
  ##   "amazon-client-secret": The client secret from amazon.developers.com
  ##
  ## NOTE: When amazonCredentials.existingSecret is set, clear text credentials passed in the previous parameters
  ## "amazonCredentials.clientId" and "amazonCredentials.clientSecret" are ignored.
  existingSecret: ""

##
## Stateful Backend Components
##
statefulRabbitMq:
  image: cognigy.azurecr.io/rabbitmq:3.9.24_cognigy-4.X
  replicaCount: 1
  resources:
    limits:
      memory: 2Gi
      cpu: "2"
    requests:
      memory: 1Gi
      cpu: "1"
  extraEnvVars: []
  ## The memory threshold under which RabbitMQ will stop reading from client network sockets, in order to avoid being killed by the OS
  ## ref: https://www.rabbitmq.com/alarms.html
  ## ref: https://www.rabbitmq.com/memory.html#threshold
  ##
  memoryHighWatermark:
    ## Enable configuring Memory high watermark on RabbitMQ
    ##
    enabled: true
    ## Memory high watermark type. Either `absolute` or `relative`
    ##
    type: "relative"
    ## Memory high watermark value.
    ## The default value of 0.4 stands for 40% of available RAM
    ## Note: the memory relative limit is applied to the statefulRabbitMq.resource.limits.memory to calculate the memory threshold
    ## You can also use an absolute value, e.g.: 256MB
    ##
    value: 0.4
  ## RabbitMQ Configuration file content: required cluster configuration
  ## ref: https://www.rabbitmq.com/configure.html#configuration-files
  ## Do not override unless you know what you are doing.
  ## To add more configuration, use `extraConfiguration` of `advancedConfiguration` instead
  ##
  configuration: |-
    {{ tpl .Values.statefulRabbitMq.extraConfiguration . }}
    ## allow access to the guest user from anywhere on the network
    ## https://www.rabbitmq.com/access-control.html#loopback-users
    ## https://www.rabbitmq.com/production-checklist.html#users
    loopback_users.guest = false

    ## Send all logs to stdout/TTY. Necessary to see logs when running in a container
    log.console = true

    {{- if .Values.statefulRabbitMq.memoryHighWatermark.enabled }}
    ## Memory Threshold
    ##
    total_memory_available_override_value = {{ include "rabbitmq.toBytes" .Values.statefulRabbitMq.resources.limits.memory }}
    vm_memory_high_watermark.{{ .Values.statefulRabbitMq.memoryHighWatermark.type }} = {{ .Values.statefulRabbitMq.memoryHighWatermark.value }}
    {{- end }}
  ## Optionally specify Configuration file content: extra configuration to be appended to RabbitMQ configuration
  ## Use this instead of `configuration` to add more configuration
  ## Example configuration:
  ## extraConfiguration: |-
  ##   listeners.tcp.default = 5672
  ##
  extraConfiguration: |-
    #default_vhost = {{ .Release.Namespace }}-vhost
    #disk_free_limit.absolute = 50MB

  ## Optionally specify Configuration file content: advanced configuration
  ## Use this as additional configuration in classic config format (Erlang term configuration format)
  ## ref: https://www.rabbitmq.com/configure.html#advanced-config-file
  ## Example configuration:
  ## advancedConfiguration: |-
  ##   [
  ##     {rabbit, [
  ##         {tcp_listeners, [5672]}
  ##       ]
  ##     }
  ##   ].
  advancedConfiguration: ""
  ## RabbitMQ pods' Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## Note: The following values of RabbitMQ pod's Security Context runAsUser and runAsGroup are required to run the pod as non-root user.
  ##
  securityContext:
    runAsUser: 1337
    runAsGroup: 1337
  affinity: {}
  nodeSelector: {}
  tolerations: []

## If redis is not required to deploy then you can set the "statefulRedis.enabled" flag to false. By default it is always enabled. 
## statefulRedis:
##   enabled: false

statefulRedis:
  enabled: true
  image: cognigy.azurecr.io/redis:7.0.8_cognigy_4.X
  replicaCount: 1
  resources:
    limits:
      memory: 512Mi
      cpu: "0.5"
    requests:
      memory: 100Mi
      cpu: "0.2"
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []

## If redis-persistent is not required to deploy then you can set the "statefulRedis.enabled" flag to false. By default it is always enabled.
## statefulRedisPersistent: 
##   enabled: false

statefulRedisPersistent:
  enabled: true
  image: cognigy.azurecr.io/redis:7.0.8_cognigy_4.X
  replicaCount: 1
  resources:
    limits:
      memory: 512Mi
      cpu: "0.5"
    requests:
      memory: 100Mi
      cpu: "0.2"
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  ## Persistence parameters
  ##
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    ## Optionally specify StorageClass for Redis Persistent data volume
    ## If defined, storageClassName: <storageClass>
    ## If undefined or set to null, storageClassName is set according to the value defined in "cloud.provider"
    ##
    storageClass: ""
    ## Optionally specify PVC Storage Request for RedisPersistent data volume
    ## Note: Default value is 10Gi
    ##
    size: ""

##
## Cognigy.AI components
##
serviceAi:
  image: cognigy.azurecr.io/service-ai:release-263bcef264-1676645217
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  ## Optionally specify list of additional volumes
  ## Examples:
  ## extraVolumes:
  ##   - name: foo
  ##     secret:
  ##       secretName: mysecret
  ##       optional: false
  ##   - name: config-volume
  ##     configMap:
  ##       name: special-config
  ##       items:
  ##       - key: SPECIAL_LEVEL
  ##         path: keys
  extraVolumes: []
  ## Optionally specify list of additional volumeMounts
  ## Examples:
  ## extraVolumeMounts:
  ##   - name: foo
  ##     mountPath: "/etc/foo"
  ##     readOnly: true
  ##   - name: config-volume
  ##     mountPath: /etc/config
  extraVolumeMounts: []
  ## Optionally specify list of extra environment variables to add to the container
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  securityContext: {}
  ## Optionally specify affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}
  ## Optionally specify node labels for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  ##
  nodeSelector: {}
  ## Optionally specify tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  ##
  tolerations: []
  ## Optionally enable HorizontalPodAutoscaler for pods
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  ## Make sure that pods have resources limits and requests defined and the 'metrics server' has been deployed and configured in the cluster.
  ## https://github.com/kubernetes-sigs/metrics-server
  ##
  ##
  ## Note: This is a experimental feature and should not be used in production. Please don't enable this flag for now if you are a
  ## customer running Cognigy.AI/Cognigy Insights using this HelmChart. This note will be removed once we are done with the
  ## testing and the feature will be production ready.
  horizontalPodAutoscaler:
    ## Whether enable horizontal pod autoscaler
    enabled: false
    ## Define the minimum allowed replicas to which the scaling target can be scaled down
    minReplicas: 3
    ## Define the maximum allowed replicas to which the scaling target can be scaled up
    maxReplicas: 10
    ## Define metrics against which HorizontalPodAutoscaler will react
    ## metrics:
    ##   - type: Resource
    ##     resource:
    ##       name: memory
    ##       target:
    ##         type: Utilization
    ##         averageUtilization: 80
    ##   - type: Resource
    ##     resource:
    ##       name: cpu
    ##       target:
    ##         type: Utilization
    ##         averageUtilization: 70
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            ## Define the CPU target to trigger the scaling actions (utilization percentage)
            averageUtilization: 70
serviceAlexaManagement:
  image: cognigy.azurecr.io/service-alexa-management:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceAnalyticsCollector:
  image: cognigy.azurecr.io/service-analytics-collector:release-a46b534-1676287051
  replicaCount: 3
  resources:
    requests:
      cpu: '0.300'
      memory: 160M
    limits:
      cpu: '0.300'
      memory: 200M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceAnalyticsConversations:
  image: cognigy.azurecr.io/service-analytics-conversations:release-c8fead1-1676287055
  replicaCount: 3
  requests:
    cpu: '0.1'
    memory: 120M
  limits:
    cpu: '0.3'
    memory: 250M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceAnalyticsOdata:
  image: cognigy.azurecr.io/service-analytics-odata:release-595bffc-1676287067
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 360M
    limits:
      cpu: '0.5'
      memory: 450M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceAnalyticsReporter:
  image: cognigy.azurecr.io/service-analytics-reporter:release-7fd8fea-1676287054
  replicaCount: 3
  resources:
    requests:
      cpu: '0.5'
      memory: 500M
    limits:
      cpu: '0.5'
      memory: 750M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceApi:
  image: cognigy.azurecr.io/service-api:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 280M
    limits:
      cpu: '0.4'
      memory: 350M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceAppSessionManager:
  image: cognigy.azurecr.io/service-app-session-manager:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceCustomModules:
  image: cognigy.azurecr.io/service-custom-modules:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.3'
      memory: 512M
    limits:
      cpu: '0.3'
      memory: 512M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceEndpoint:
  image: cognigy.azurecr.io/service-endpoint:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 120M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceExecution:
  image: cognigy.azurecr.io/service-execution:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '1'
      memory: 240M
    limits:
      cpu: '1'
      memory: 300M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  horizontalPodAutoscaler:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
serviceFunctionExecution:
  image: cognigy.azurecr.io/service-function-execution:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '1'
      memory: 512M
    limits:
      cpu: '2'
      memory: 512M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  horizontalPodAutoscaler:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
serviceFunctionScheduler:
  image: cognigy.azurecr.io/service-function-scheduler:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  horizontalPodAutoscaler:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
serviceHandover:
  image: cognigy.azurecr.io/service-handover:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceHttp:
  image: cognigy.azurecr.io/service-http:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.1'
      memory: 75M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  horizontalPodAutoscaler:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
serviceInsightsApi:
  image: cognigy.azurecr.io/service-insights-api:release-f959cf6-1676287051
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.1'
      memory: 75M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceCollector:
  image: cognigy.azurecr.io/service-collector:release-b4dd277-1676287054
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.1'
      memory: 75M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceInsightsUi:
  image: cognigy.azurecr.io/service-insights-ui:release-fb9a60e-1677071688
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceJourneys:
  image: cognigy.azurecr.io/service-journeys:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 100M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceLogs:
  image: cognigy.azurecr.io/service-logs:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 100M
    limits:
      cpu: '0.5'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpMatcher:
  image: cognigy.azurecr.io/service-nlp-matcher:release-8ca5978a89-1675931678
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 300M
    limits:
      cpu: '0.5'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpNer:
  image: cognigy.azurecr.io/service-nlp-ner:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.3'
      memory: 100M
    limits:
      cpu: '1.0'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  horizontalPodAutoscaler:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
serviceParser:
  image: cognigy.azurecr.io/service-parser:release-554bc4b5f3-1674818304
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
servicePlaybookExecution:
  image: cognigy.azurecr.io/service-playbook-execution:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceProfiles:
  image: cognigy.azurecr.io/service-profiles:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  horizontalPodAutoscaler:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
serviceResources:
  image: cognigy.azurecr.io/service-resources:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 512M
    limits:
      cpu: '0.5'
      memory: 512M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceRuntimeFileManager:
  image: cognigy.azurecr.io/service-runtime-file-manager:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  horizontalPodAutoscaler:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
serviceSecurity:
  image: cognigy.azurecr.io/service-security:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 60M
    limits:
      cpu: '0.4'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceSessionStateManager:
  image: cognigy.azurecr.io/service-session-state-manager:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
  horizontalPodAutoscaler:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
serviceStaticFiles:
  image: cognigy.azurecr.io/service-static-files:release-a4be4919fe-1675859489
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceTaskManager:
  image: cognigy.azurecr.io/service-task-manager:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceTrainer:
  image: cognigy.azurecr.io/service-trainer:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceUi:
  image: cognigy.azurecr.io/service-ui:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceWebchat:
  image: cognigy.azurecr.io/service-webchat:release-56efad9941-1676559429
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpQaDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-qa-de:release-6b2b8d8af1-1672750783
  replicaCount: 1
  resources:
    requests:
      cpu: '1'
      memory: 3G
    limits:
      cpu: '1'
      memory: 3G
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpScoreDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpTrainDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpQaEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-qa-en:release-6b2b8d8af1-1672750783
  replicaCount: 1
  resources:
    requests:
      cpu: '1'
      memory: 3G
    limits:
      cpu: '1'
      memory: 3G
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpScoreEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-en:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpTrainEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-en:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpQaGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-qa-ge:release-6b2b8d8af1-1672750783
  replicaCount: 1
  resources:
    requests:
      cpu: '1'
      memory: 3G
    limits:
      cpu: '1'
      memory: 3G
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpScoreGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ge:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 3000M
    limits:
      cpu: '2'
      memory: 4000M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpTrainGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ge:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 3000M
    limits:
      cpu: '2'
      memory: 4000M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpScoreJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ja:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpTrainJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ja:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpScoreKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ko:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpTrainKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ko:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpScoreXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpTrainXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp:release-aa90a88978-1675423326
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpEmbeddingEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-embedding-en:release-3bde3dd479-1665154702
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 2Gi
    limits:
      cpu: '0.4'
      memory: 3Gi
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpEmbeddingXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-embedding-xx:release-3bde3dd479-1665154702
  replicaCount: 2
  resources:
    requests:
      cpu: '0.4'
      memory: 1.2Gi
    limits:
      cpu: '0.8'
      memory: 2Gi
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpEmbeddingGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-embedding-ge:release-3bde3dd479-1665154702
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 3Gi
    limits:
      cpu: '1.0'
      memory: 4Gi
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierScoreEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-en:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 150M
    limits:
      cpu: '1.0'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierTrainEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-en:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 300M
    limits:
      cpu: '1.0'
      memory: 1000M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpOrchestrator:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-orchestrator:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 700M
    limits:
      cpu: '1.0'
      memory: 1200M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierScoreGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ge:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 1G
    limits:
      cpu: '1.0'
      memory: '1.5G'
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierTrainGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ge:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 1G
    limits:
      cpu: '1.0'
      memory: '1.5G'
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierScoreDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 150M
    limits:
      cpu: '1.0'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierTrainDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 300M
    limits:
      cpu: '1.0'
      memory: 1000M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierScoreJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ja:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 150M
    limits:
      cpu: '1.0'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierTrainJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ja:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 300M
    limits:
      cpu: '1.0'
      memory: 1000M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierScoreKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ko:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 150M
    limits:
      cpu: '1.0'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierTrainKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ko:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 300M
    limits:
      cpu: '1.0'
      memory: 1000M
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierScoreXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 1G
    limits:
      cpu: '1.0'
      memory: '1.5G'
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []
serviceNlpClassifierTrainXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier:release-6780c73ffd-1675844159
  replicaCount: 2
  resources:
    requests:
      cpu: '0.3'
      memory: 1G
    limits:
      cpu: '1.0'
      memory: '1.5G'
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []

## You can enable the pod monitor if any prometheus instance is ruinning on your cluster.
podMonitors:
  enabled: false
  ## The namespace for the pod-monitor should be the same namespace where your prometheus instance is running
  namespace: ""

# Anti Virus Scanning
clamd:
  # 4000M is the maximum
  streamMaxLength: 4000M
  maxScanSize: 300M
  maxFileSize: 100M
  tcpAddr: 0.0.0.0
  image: cognigy.azurecr.io/clamav:0.105
  replicaCount: 1
  resources:
    limits:
      cpu: 1000m
      memory: 3000Mi
    requests:
      cpu: 800m
      memory: 2000Mi
  extraEnvVars: []
  securityContext: {}
  affinity: {}
  nodeSelector: {}
  tolerations: []

# The values below are used for Traefik Helm chart. For more information, see
# https://github.com/traefik/traefik-helm-chart
traefik:
  enabled: true
  fullnameOverride: traefik
  image:
    name: cognigy.azurecr.io/traefik
    tag: "2.6.3"
    pullPolicy: IfNotPresent
  deployment:
    ## Specify imagePullSecrets to pull the image from private repository.
    ## Based on the information provided in "imageCredentials" parameter previously, this should be
    ## either "cognigy-registry-token" or predefined secrets.
    ## NOTE: Can be ignored if traefik is not enabled.
    imagePullSecrets:
      - name: cognigy-registry-token
    replicas: 3
  logs:
    general:
      level: INFO
    access:
      enabled: true
      filters: {}
      fields:
        general:
          defaultmode: keep
          names: {}
        headers:
          defaultmode: drop
          names: {}
  ingressClass:
    enabled: true
    isDefaultClass: true
    fallbackApiVersion: ""
  globalArguments: []
  additionalArguments:
    - "--api.insecure=true"
    - "--entryPoints.web.forwardedHeaders.insecure"
    - "--entryPoints.websecure.forwardedHeaders.insecure"
    - "--entryPoints.web.proxyProtocol.insecure"
    - "--entryPoints.websecure.proxyProtocol.insecure"
  ports:
    traefik:
      port: 9000
      expose: false
      exposedPort: 9000
      protocol: TCP
    web:
      port: 8000
      expose: true
      exposedPort: 80
      protocol: TCP
      ## NOTE: If traefik is enabled ("traefik.enabled: true"), and you provide "tls.enable: false", then the auto redirection of http to https
      ## also must be disabled by setting traefik.ports.web.redirectTo: null
      redirectTo: websecure
    websecure:
      port: 8443
      expose: true
      exposedPort: 443
      protocol: TCP
      tls:
        enabled: true
        options: ""
        certResolver: ""
        domains: []
    metrics:
      port: 9100
      expose: false
      exposedPort: 9100
      protocol: TCP
  service:
    enabled: true
    type: LoadBalancer
    annotations: {}
    annotationsTCP: {}
    annotationsUDP: {}
    labels: {}
    spec: {}
    loadBalancerSourceRanges: []
    externalIPs: []
  tlsOptions:
    default:
      minVersion: VersionTLS12
      cipherSuites:
        - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
        - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
        - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384

## For the securityContext config of traefik, please refer to the official values.yaml file of the traefik
## This is the link for chart v10.19.4 for example
## https://github.com/traefik/traefik-helm-chart/blob/f24ac3c53579e0889b53a29f23a76d359ad54803/traefik/values.yaml#L490-L501
