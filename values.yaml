# If cognigy provided mongodb helm chart(https://github.com/Cognigy/cognigy-mongodb-helm-chart) is not in use then set "mongodb.enabled" to false. It will skip doing all corresponding task related to mongodb helm chart, such as creating necessary database. By default it is always enabled.
# mongodb:
#   enabled: false

# This MongoDB user and password should have the permission to create users and databases, so normally it is admin or root
# It does NOT have to be root user. We use these key names to be compatible with Bitnami MongoDB Helm Chert
mongodb:
  enabled: true 
  auth:
    rootUser: root
    rootPassword: ""
    ## Existing secret with MongoDB credentials. Mandatory keys: `username` and `password`, that contains the value 
    ## of "rootUser" and "rootPassword"
    ## NOTE: When it's set the previous parameters "rootUser" and "rootPassword" are ignored.
    ##
    existingSecret: ""
  # Default value for the MongoDB replica set deployed by Cognigy MongoDB Helm Chart into 3 availability zones:
  # hosts: mongodb-0.mongodb-headless.mongodb.svc.cluster.local:27017,mongodb-1.mongodb-headless.mongodb.svc.cluster.local:27017,mongodb-2.mongodb-headless.mongodb.svc.cluster.local:27017
  # for development and testing purposes with a single replica MongoDB installation in "mongodb" namespace use:
  # hosts: "mongodb-0.mongodb-headless.mongodb.svc.cluster.local:27017"
  hosts: mongodb-0.mongodb-headless.mongodb.svc.cluster.local:27017,mongodb-1.mongodb-headless.mongodb.svc.cluster.local:27017,mongodb-2.mongodb-headless.mongodb.svc.cluster.local:27017

## Credentials for pulling image from private image registry.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
## NOTE 1: Either clear text credentials (registry, username and password) or pullSecrets must be provided.
## NOTE 2: If traefik is enabled and you provide clear text credentials, then traefik.deployment.imagePullSecrets must
## be set to "cognigy-registry-token". If you set custom pullSecrets value instead, set the same value under traefik.deployment.imagePullSecrets
imageCredentials:
  ## Alternatively specify the username, password and the url of the private registry.
  ## A kubernetes.io/dockerconfigjson type secret named "cognigy-registry-token" will be created based on these information.
  registry: "cognigy.azurecr.io"
  username: ""
  password: ""

  ## Alternatively specify an array of imagePullSecrets.
  ## Secrets must be manually created in the proper namespace beforehand.
  ## Example:
  ## pullSecrets:
  ##   - cognigyRegistrySecretName
  ##
  ## NOTE: When registry, username and password all are set, the pullSecrets are ignored.
  pullSecrets: []

# Cognigy supports 3 cloud providers:
# - aws
# - azure
# - generic for on-premises installation e.g. with OpenShift
cloud:
  provider: aws
  region: ""

# For AWS cloud provider only:
efs:
  flowModules:
    id: ""
  functions:
    id: ""

cognigyLiveAgent:
  platformToken: ""
  ## Existing secret with live-agent credentials. The secret must have the following key:
  ##   "cognigy-live-agent-platform-token": The token for cognigy live agent
  ##
  ## NOTE: When cognigyLiveAgent.existingSecret is set, clear text token passed in the previous parameter
  ## "cognigyLiveAgent.platformToken" is ignored.
  existingSecret: ""

# Install Management UI on the cluster.
# It is not required, since if not installed and the API endpoint is accessible from the Internet, you can still use the provided Management UI at https://management-ui-v4.cognigy.ai/
managementUi:
  enabled: false
  ingress:
    enabled: true
    host: ""
  image: cognigy.azurecr.io/management-ui:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 1
  resources:
    limits:
      memory: "30Mi"
      cpu: "20m"
    requests:
      memory: "10Mi"
      cpu: "10m"
  extraEnvVars: []

## managementUiCredentials: '[{"username": "example_username", "password": "example_password"}]'
managementUiCredentials: '[]'
## The name of an existing secret with management UI credentials. The secret must have a
## "management-ui-creds.json" key from where the password will be extracted and the content of that key 
## should be in the following format-
##    [{"username": "user", "password": "pass"}, {"username": "user2", "password": "pass2"}]
## NOTE: When this is set, clear text credentials passed in the variable "managementUiCredentials" is ignored.
managementUiCredentialsExistingSecret: ""


## SMTP server information for 'forgot password' functionality.
## Password for the SMTP server.
## A secret named "cognigy-smtp" will be created based on the information provided.
smtpPassword: ""
## The name of an existing secret with SMTP server credentials. The secret must have a
## "system-smtp-password" key from where the password will be extracted.
## NOTE: When this is set, "smtpPassword" is ignored.
smtpPasswordExistingSecret: ""

# This is a temp flag for "Cognigy Apps" which will be removed once we are done with
# fully finishing this feature. Please don't use this flag for now if you are a
# customer running Cognigy.AI/Cognigy Insights using this HelmChart.
cognigyApps:
  enabled: false

## Traefik TLS certificate for the hostname defined at ingress.<service_name>.host
## NOTE: If you provide "tls.enable: true" and "traefik.enabled: true", either tls.crt and tls.key or tls.existingSecret must be provided.
tls:
  ## Enable traefik tls
  ## NOTE: If traefik is enabled ("traefik.enabled: true"), and you provide "tls.enable: false", then the auto redirection of http to https
  ## also must be disabled by setting traefik.ports.web.redirectTo: null
  enabled: true
  ## Add Custom CA certificate. A tls type secret named "cognigy-traefik" will be created based on the values of tls.crt and tls.key
  ## Careful with the indentation
  ## For more information, see https://helm.sh/docs/chart_template_guide/yaml_techniques/#strings-in-yaml
  ##
  ## Custom CA certificate in plaintext, not base64 encoded.
  ## Example:
  ##   crt: |
  ##     -----BEGIN CERTIFICATE-----
  ##     -----END CERTIFICATE-----
  crt: ""
  ## CA certificate private key in plaintext, not base64 encoded.
  ## Example:
  ## key: |
  ##   -----BEGIN PRIVATE KEY-----
  ##   -----END PRIVATE KEY-----
  key: ""
  ## Existing secret with TLS certificates. The secret must have the following two keys:
  ## "tls.crt": Containing the CA certificate
  ## "tls.key": Containing the certificate key
  ## NOTE: When tls.existingSecret is set, clear text certificate passed in the previous parameters "tls.crt" and "tls.key" are ignored.
  existingSecret: ""

# If ingress is not required to deploy then you can set "ingress.enabled" to false. By default it is always enabled.
# ingress:
#   enabled: false
ingress:
  enabled: true
  serviceAnalyticsOdata:
    host: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceApi:
    host: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceAppSessionManager:
    host: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceEndpoint:
    host: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceInsightsApi:
    host: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceRuntimeFileManager:
    host: ""
    ipWhiteListMiddleware: 
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceUi:
    host: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceStaticFiles:
    host: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0
  serviceWebchat:
    host: ""
    ipWhiteListMiddleware:
      enabled: true
      ipWhiteList:
        sourceRange:
          - 0.0.0.0/0
        ipStrategy:
          depth: 0

## Kubernetes service type
##
service:
  serviceAnalyticsOdata:
    ## Optional Service annotations.
    ## Example:
    ## annotations:
    ##   service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
    ##   service.name: service-analytics-odata
    ##
    annotations: {}
  serviceApi:
    annotations: {}
  serviceAppSessionManager:
    annotations: {}
  serviceEndpoint:
    annotations: {}
  serviceInsightsApi:
    annotations: {}
  serviceInsightsUi:
    annotations: {}
  serviceRuntimeFileManager:
    annotations: {}
  serviceStaticFiles:
    annotations: {}
  serviceUi:
    annotations: {}
  serviceWebchat:
    annotations: {}
  statefulRabbitMq:
    annotations: {}
  statefulRedis:
    annotations: {}
  statefulRedisPersistent:
    annotations: {}
  serviceNlpEmbeddingEn:
    annotations: {}
  serviceNlpEmbeddingXx:
    annotations: {}
  serviceNlpEmbeddingGe:
    annotations: {}

cognigyEnv:
  NODE_ENV: production

  # Enable the creation of metrics which will then get consumed
  # by our 'service-monitoring'.
  MONITOR_RPC_CALLS: "true"

  # Redis configuration, will soon be a connection string
  REDIS_HOST: redis
  REDIS_PORT: "6379"

  # Redis (persistent) configuration, will soon be a connection string
  REDIS_PERSISTENT_PORT: "6379"
  REDIS_PERSISTENT_HOST: redis-persistent

  # limits (api requests, db queries, context-size)
  MESSAGE_TTL_SECONDS: "120"
  MAX_MEMORY_OBJECT_SIZE: "65536"
  HTTP_JSON_BODY_LIMIT: "65536"
  MAX_BYTE_SIZE: "524288"
  RESPONSE_BYTES_LIMIT: "524288"

  # features (enable / disable)
  FEATURE_CUSTOM_NODES: "true"

  # log cleanup
  LOG_ENTRIES_TTL_IN_MINUTES: "1440"
  LOG_ENTRIES_BUFFER_IN_SECONDS: "5"

  # SMTP server for 'forgot password' functionality
  SYSTEM_SMTP_HOST: "test"
  SYSTEM_SMTP_PORT: "test"
  SYSTEM_SMTP_USERNAME: "test"
  SYSTEM_SMTP_FROM: "test"
  SYSTEM_SMTP_CONNECTION_TYPE: "starttls"
  # SYSTEM_SMTP_PASSWORD is a secret!

  # Domains to whitelist for cors for the API (service-api)
  API_CORS_WHITELIST: ""

  # Execution relevant configuration (service-execution)
  MODULE_MAX_EVENT_EMISSIONS: "10"
  MAX_MODULE_EXECUTION_TIME_IN_SECONDS: "20"

  # Endpoint configuration for Alexa
  ALEXA_END_SESSION_AFTER_EACH_REPLY: "true"

  # Enable max contact profile TTL in minutes
  MAX_CONTACT_PROFILE_TTL_IN_MINUTES: "43200"

  # Enable max conversation TTL in minutes
  MAX_CONVERSATION_TTL_IN_MINUTES: "43200"

  # Enable max conversation TTL in minutes
  MAX_SESSION_STATE_TTL_IN_MINUTES: "10080"

  # Enable new Cognigy Insights UI
  FEATURE_USE_SERVICE_INSIGHTS_UI: "true"

  # Path to files containing pre-computed noise embeddings
  NLP_PRECOMPUTED_EMBEDDINGS_PATH: "/embedding/precomputed_embeddings"

  ### Platform Integration ENV Variables ###
  
  # A link to an online instance of the adaptive cards designer
    # ADAPTIVE_CARDS_DESIGNER_URL:
  
  # Agent Bot description for Live Agent, Chatwoot, etc. 
    # AGENT_BOT_DESCRIPTION:
  
  # Agent Bot name for Live Agent, Chatwoot, etc.
    # AGENT_BOT_NAME:

  # Additional Alexa configuration
    # ALEXA_END_SESSION_AFTER_EACH_REPLY:
  
  # Amazon client credetentials
    # AMAZON_CLIENT_ID:
    # AMAZON_CLIENT_SECRET:

  # Check avaialability node timeout of agent   
    # CHECK_AGENT_AVAILABILITY_NODE_TIMEOUT_IN_SECONDS:

  # Cognigy Live Agent (Chatwoot based)
    # CLIENT_ID_COGNIGY_LIVE_AGENT:

  # Cognigy LiveAgent setup
    # COGNIGY_LIVE_AGENT_API_BASE_URL_WITH_PROTOCOL:
    # COGNIGY_LIVE_AGENT_PLATFORM_TOKEN:

  # RPC timeout for creating runtime file in seconds. Default value is 8 secs. 
    # CREATE_RUNTIME_FILE_TIMEOUT_IN_SECONDS:

  # Redis default cache TTL
    # DEFAULT_CACHE_TTL:

  # Endpoint base URL 
    # ENDPOINT_BASE_URL_TUNNEL:

  # URL pointing to the Endpoint service
    # ENDPOINT_BASE_URL_WITH_PROTOCOL:

  # Allows our customers to specify CORS origins. If you want to specify CORS origins,
  # add all of your origins into a comma-delimited list as the following examples:
  # "http://example1.com, http://example2.com", "http://*.example3.com"
    # ENDPOINT_CORS_WHITELIST:

  # The TTL of the session storage in the endpoint transformers in seconds.
    # ENDPOINT_TRANSFORMER_SESSION_STORAGE_TTL:

  # TTL for user meta-data the system uses to keep track, such as flow changes
    # ENDPOINT_USER_METADATA_REDIS_TTL:

  # Additional facebook configuration. This is to verify the token in the webhook
    # FB_VERIFY_TOKEN:

  # Enable the "Follow Sessions" feature
    # FEATURE_ENABLE_FOLLOW_SESSION:

  # Enable new voicegateway 2 endpoint (Jambonz) by setting this to true
  # and adding organisationId to the list in FEATURE_ENABLE_VOICEGATEWAY_2_WHITELIST below:
    # FEATURE_ENABLE_VOICEGATEWAY_2:

  # Comma-separated list of organisationId to enable voicegateway 2
  # As an example: FEATURE_ENABLE_VOICEGATEWAY_2_WHITELIST=5f99a7ad6107a6be813ff301,5f99a7ad6107a6be813ff302
    # FEATURE_ENABLE_VOICEGATEWAY_2_WHITELIST:

  # Define a whitelist of Endpoints that will not be shown in the UI, seperated by comma.
  # Use the actual endpoint channel type values, e.g. webchat2
  # Example: FEATURE_OMMITED_ENDPOINTS_FROM_UI="twilio,twilio-autopilot,twilio-sms,webchat2"
    # FEATURE_OMMITED_ENDPOINTS_FROM_UI:

  # Enable detailed logs for RPC calls handling
    # FEATURE_RPC_LOGS:

  # Enable Central Configuration for Adaptive cards
    # FEATURE_USE_ADVANCED_ADAPTIVECARDS_INTEGRATION:

  # Whether Cognigy Live Agent should be activated
    # FEATURE_USE_COGNIGY_LIVE_AGENT:'true'

  # Enable the SSO features and show the Microsoft nodes if enabled
    # FEATURE_USE_MICROSOFT_SSO:

  # Activate possibility of Non-Conversational endpoint creation
    # FEATURE_USE_NON_CONVERSATIONAL_ENDPOINT:

  # Enable assured message delivery for sockentendpoint
    # FEATURE_USE_SOCKETENDPOINT_EVENTBUFFER:

  # If set to "true", will exchange "Cogngiy.AI" for a generic name
    # FEATURE_USE_WHITELABELING:

  # Cleanup/remove idle socket client connection
    # GENESYS_CLOUD_SOCKET_CLIENT_CLEANUP_INTERVAL:

  # To limit the number of unacknowledged messages on a channel/connection on AMQP client 
    # HANDOVER_POLLING_PREFETCH:

  # Salesforce
    # HANDOVER_POLLING_TTL:

  # The TTL for the RPC call to fetch the conversation. 
  # Should be increase if the save interval for analytics is increased
    # HANDOVER_READ_CONVERSATION_TIMEOUT_IN_SECONDS:

  # Handover TTL for persisting agent bot id into redis  
    # HANDOVER_TTL:

  # the JWT token to decode 'realtime-tokens'
    # JWT_SECRET:

  # CORS whitelist for the live-agent API
    # LIVE_AGENT_API_CORS_WHITELIST:

  # Live Agent API secret
    # LIVE_AGENT_API_SECRET:

  # Live Agent base URL
    # LIVE_AGENT_BACKEND_BASE_URL_WITH_PROTOCOL:

  # Message displayed when handover session is aborted 
    # LIVE_AGENT_LITE_EXPIRED_MESSAGE:

  # Time after which handover requests should automatically be closed in minutes. Defaults to 30. 0 means "no cleanup"
    # LIVE_AGENT_LITE_HANDOVER_TTL:

  # The TTL of the JWT + access token in seconds
    # LIVE_AGENT_TOKEN_TTL_IN_SECONDS:

  # Register a rotue for loader.io verification
    # LOADER_IO_TOKEN:

  # The TTL of messages in the endpoint-session-queues in seconds.
    # MESSAGE_TTL_SECONDS:120 

  # Redis key TTL which stores inject/notify meta data 
    # NOTIFY_INJECT_EXPIRATION_IN_SECONDS:

  # The key for the built-in pendo. If set, pendo routines will be activated.
    # PENDO_KEY:

  # To reference assets in the `public` folder
    # PUBLIC_URL:

  # Verify token for RCE 
    # RCE_VERIFY_TOKEN:

  # A comma-separated list of Snapshots that are deployed as Snapshot Executors
    # SNAPSHOTS_USING_SNAPSHOT_EXECUTOR:

  # TTL for the Redis key used to handle client instances 
    # SOCKET_ENDPOINT_REDIS_CLIENT_TTL:

  # TTL for the Redis key used to handle buffer events
    # SOCKET_ENDPOINT_REDIS_CONNECTION_STATUS_TTL:

  # TTL for the Redis key used to handle sockets
    # SOCKET_ENDPOINT_REDIS_SOCKET_TTL:

  # Temporary workaround for a customer:
  # Wait for n seconds before we actually try to read the conversation history
  # as it seems that the history is not complete and
  # that 'waitForPossibleInput' does not work properly
    # TEMP_FIX_WAIT_FOR_CONVERSATION_HISTORY:

  # TWITTER configuration
    # TWITTER_ACCESS_TOKEN:
    # TWITTER_ACCESS_TOKEN_SECRET:
    # TWITTER_APIKEY:
    # TWITTER_CONSUMER_KEY:
    # TWITTER_CONSUMER_SECRET:
    # TWITTER_FLOWNAME:
    # TWITTER_USERNAME:

  # Comma-separated list of origin URIs that are allowed to embed the webchat page as an iframe
  # You can use * in URLs to define subdomains or just * to allow any source
  # Example: WEBCHAT_EMBEDDER_WHITELIST=mywebsite.com,myotherwebsite.com
  # Example: WEBCHAT_EMBEDDER_WHITELIST=*.mywebsite.com
  # Example: WEBCHAT_EMBEDDER_WHITELIST=* 
    # WEBCHAT_EMBEDDER_WHITELIST:

  # Prevent websocket connections from switching to long polling if set to true.
    # WEBCHAT_FORCE_WEBSOCKETS:'false'

  # WhatsApp cloud API url, whereas undefined means default url `https://graph.facebook.com/v13.0`
    # WHATS_APP_GRAPH_API_HOST:

  # Workplacey by facebook specific
    # WORKPLACE_VERIFY_TOKEN:
    # WORKPLACE_APP_ID:
    # WORKPLACE_APP_REDIRECT:
    # WORKPLACE_APP_SECRET:

  ### service-ai env variables ###

  # max number of queues for consistent-hash-exchange in service-ai. default is 20.
  # AI_MESSAGE_QUEUES_AMOUNT: "20"

  # queue TTL in seconds Queues will expire after a period of time
  # only when they are not used (e.g. do not have consumers). default is 30 seconds.
  # AI_MESSAGE_QUEUES_TTL_IN_SECONDS: "30"

  # maximum byte size for context, contact profiles, input data
  # MAX_MEMORY_OBJECT_SIZE: ""

  # defaults to 'false', needs to be set to 'true'
  # FEATURE_ALLOW_ADDITIONAL_ACTIONS_IN_CODE_NODES: ""

  # maximum loops to allow in Flows
  # MAX_LOOPS: "4"

  # Use NLU 2.0 for short utterances
  # SHORT_UTTERANCES_V2: ""

  # Can be set to 0 to never expire session states
  # MAX_SESSION_STATE_TTL_IN_MINUTES: "0"

  # The TTL of the session state in redis. Default is 10 minutes
  # SESSION_STATE_REDIS_TTL_IN_SECONDS: "600"

  # The TTL of the session storage in the NLU transformers in seconds.
  # NLU_TRANSFORMER_SESSION_STORAGE_TTL: ""

  # The max outgoing hhtp requests that can be executed in a NLU transformer
  # NLU_TRANSFORMER_MAX_OUTGOING_REQUESTS: ""

  # The amount of times we will retry sending an email
  # SMTP_RETRY_ATTEMPTS: ""

  # The maximum attachment size allowed when sending an email
  # SMTP_MAX_ATTACHMENT_SIZE: ""

  # The amount of entries to be buffered, till the batch get sent
  # BULK_CREATE_TRAINER_MAX_BATCH_SIZE: ""

  # Timeout for 'getNluResultsRpc' call
  # RPC_TIMEOUT_GET_NLU_RESULTS_IN_MS: "5000"

  # The path to the HTMLTemplate for the email notification Node.
  # if not provided the value defaults to cognigy internal email html template
  # EMAIL_NOTIFICATION_HTML_TEMPLATE_FILE_PATH: ""

  # Maximum size for Fuzzy Search Node source data (in bytes)
  # FUZZYSEARCH_MAX_OBJECT_SIZE: ""

  # The amount of messages one AI should handle in parallel
  # AI_MESSAGE_PREFETCH_COUNT: ""

  # Whether to use the new queueing concept that better handles messages in parallel
  # FEATURE_USE_QUEUEING_V2: ""

  # Controls the LRU cache for the chart executable
  # AI_LRU_CACHE_CHART_EXECUTABLE_ENABLED: "true"
  # AI_LRU_CACHE_CHART_EXECUTABLE_MAX_SIZE: "1000"
  # AI_LRU_CACHE_CHART_EXECUTABLE_MAX_AGE_IN_SECONDS: "86400"

  # Controls the LRU cache for the project data
  # AI_LRU_CACHE_PROJECT_ENABLED: "true"
  # AI_LRU_CACHE_PROJECT_MAX_SIZE: "1000"
  # AI_LRU_CACHE_PROJECT_MAX_AGE_IN_SECONDS: "86400"

  # Controls the LRU cache for connections
  # AI_LRU_CACHE_CONNECTIONS_MAX_AGE_IN_SECONDS: "86400"
  # AI_LRU_CACHE_CONNECTIONS_ENABLED: "true"
  # AI_LRU_CACHE_CONNECTIONS_MAX_SIZE: "1000"

  # Whether to refresh the Profile on each input
  # AI_REFRESH_PROFILES_ENABLED: ""

  # Max timeout fot the loadSessionStateRpc call
  # AI_LOAD_SESSION_STATE_RPC_TIMEOUT_IN_SECONDS: "2"

  # Timeout for HTTP requests for the httpRequest Node
  # HTTP_NODE_TIMEOUT_IN_SECONDS: ""

  # The TTL of Brains in AI. Default is 10 minutes
  # AI_BRAIN_TTL: ""

  # The cleanup interval of Brains in AI. Default is 30s
  # AI_BRAIN_CLEANUP_INTERVAL: ""

  # Timeout for Question node datepicker function.
  # AI_DATEPICKER_FUNCTION_VM_TIMEOUT_IN_MS: "150"

  ### service-execution env variables ###

  # Default value is 512 MiB
  # MAX_EXTENSIONS_CACHE_DIR_SIZE_IN_MB: "512"

  # Path to extensions cache directory
  # PATH_TO_EXTENSIONS_CACHE_DIR: ""

  # Amount of extensions that will be dropped from extensions map if the max dir size exceeds
  # AMOUNT_TO_DROP_IF_MAX_EXTENSIONS_DIR_SIZE_EXCEEDS: ""

  # Whether Extensions should be executed in a VM. Default is true
  # EXECUTE_EXTENSIONS_IN_VM: "true"

  ### service-function-execution env variables ###

  # The total number of http requests a customer can send via the httpRequest API
  # FUNCTION_MAX_OUTGOING_HTTP_REQUESTS: ""

  # The total amount of data (in bytes) a http response can contain - used for the
  # httpRequest API
  # FUNCTION_MAX_HTTP_REQUEST_RESPONSE_SIZE_BYTES: ""

  # The maximum amount of minutes a function instance is allowed to run
  # FUNCTION_EXECUTION_MAX_EXECUTION_TIME_IN_MINUTES: ""

  ### service-function-scheduler env variables ###

  # how long should we store instance information in the database -
  # we use a time-based index in order to rotate the data
  # FUNCTION_INSTANCE_EXPIRATION_IN_MINUTES: ""

  # the maximum number of active/running function instances per
  # organisation
  # FUNCTION_INSTANCE_MAX_RUNNING_NUMBER_PER_ORGANIZATION: ""

  # the maximum size (in bytes) for the parameters object with which
  # a function instance can be started, default 128 KB
  # FUNCTION_PARAMETERS_SIZE_MAX_IN_BYTES: ""

  ### service-http env variables ###

  # The amount of messages service-http can handle in parallel
  # SERVICE_HTTP_PREFETCH: ""

  # The max limit of a response object in bytes
  # RESPONSE_BYTES_LIMIT: ""

  ### service-parser env variables ###
  
  # The timeout for rpc calls made by service-parser
  # SERVICE_PARSER_RPC_TIMEOUT_IN_SEC: "20"

  # The chunk size for intent sentences to send to service-resources
  # SERVICE_PARSER_INTENT_SENTENCE_CHUNKSIZE: "300"

  # The chunk size for lexicon entries to send to service-resources
  # SERVICE_PARSER_LEXICON_ENTRY_CHUNKSIZE: "300"

  # The max amount of lexicon entries service-parser can import
  # SERVICE_PARSER_MAXIMUM_LEXICONS_ENTRIES_IMPORT: "1000000"

  ### service-playbook-execution env variables ###

  # Maximum excutions Default=10
  # MAX_CONCURRENT_PLAYBOOK_EXECUTIONS: "10"

  ### service-profiles env variables ###

  # Can be set to 0 to never expire contact profiles
  # MAX_CONTACT_PROFILE_TTL_IN_MINUTES: "0"

  # maximum number of unacknowledged messages in Events queues
  # SERVICE_EVENTS_PREFETCH: "100"

  ### service-runtime-file-manager env variables ###

  # Allows our customers to specify CORS origins. If you want to specify CORS origins,
  # add all of your origins into a comma-delimited list like so:
  # "http://example1.com, http://example2.com"
  # RUNTIME_FILE_MANAGER_CORS_WHITELIST: ""

  # Max file size of a runtime file in bytes. default =1024*1024*10 (10MB) 
  # RUNTIME_FILE_MANAGER_MAX_FILE_SIZE: "10485760"

  ### service-session-state-manager env variables ###
 
  # The amount of time to buffer events before saving them. Can be set to 0 to disable buffering
  # SESSION_STATE_MANAGER_BUFFER_TIME_IN_SECONDS: "10"

  # The amount of events to buffer before saving them. Can be set to 0 to disable buffering
  # SESSION_STATE_MANAGER_BUFFER_COUNT: 100

  ### service-nlp env variables ###

  # Optional log level string: INFO | DEBUG | ERROR
  # NLP_LOG_LEVEL: ""

  # The amqp client prefetch count for parallel processing of TRAIN messages.
  # NLP_PREFETCH_COUNT: "1"

  # The amqp client prefetch count for parallel processing of SCORE messages
  # NLP_SCORE_PREFETCH_COUNT: "5"

  # The maximum number of classes considered for deep training. Defaults to 2000.
  # NLP_MAX_STATE_CONDITION_TRAINING_CLASSES: ""

  # The maximum number of classes considered for deep training. Defaults to 100.
  # NLP_STATE_CONDITION_MASKED_CLASSIFICATION_THRESHOLD: ""

  # The maximum length of inputs/example sentences for exact matching. Defaults to 2048.
  # NLP_MAX_MATCHER_LEN: ""

  # Whether to use featurization script or not. Defaults to False.
  # NLP_ENCODER_SCRIPT: ""

  # Score cache TTL in seconds. Defaults to 600.
  # NLP_CACHE_TTL: ""

  # Score ttl cache size. Defaults to 128.
  # NLP_CACHE_SIZE: ""

  # NLP_SCORE_DISABLE_CACHE_EVICTION="False"

  # Maximum size of a 'small' training batch for direct featurization. Defaults to 20.
  # NLP_SMALL_TRAINING_BATCH_SIZE: ""

  # Use the legacy ttl cache for intent train groups. New caching as opt-out. Defaults to False
  # NLP_SCORE_USE_PRIORITY_EVICTION_CACHE: ""

  # The percentage of available memory the traingroup cache can use. Defaults to 75.
  # NLP_SCORE_CACHE_PERCENTAGE_OF_AVAILABLE_MEMORY: ""

  # Interval for recalculating eviction priority and removal of unused data. Defaults to 60
  # NLP_SCORE_CACHE_UPDATE_INTERVAL_IN_MINUTES: ""

  # How many interval timeframes are used for recalculating eviction priority and removal of unused data. Defaults to 24
  # NLP_SCORE_CACHE_AMOUNT_OF_TIMEFRAMES: ""

  # Log memory consumption on every hourly update. Defaults to False
  # NLP_SCORE_LOG_CACHE_MEM: ""

  # Not case sensitive.
  # "All" when all possible Any Slot matches should be returned. 
  # "Exact" when only the exact Any Slot match should be returned, i.e. the exact sentence 
  # structure with the Any Slot.
  # Defaults to "Default".
  # Default behaviour of Any Slot matching is to return the last found match, regardless of the exact sentence structure.
  # NLP_ANYSLOT_RETURN_MODE: "Default"

  # "True" to enable the logger to print timing information in some log messages, defaults to "False"
  # NLP_ENABLE_PERFORMANCE_LOGS: "True"

  # "True" to print debug log messages as info log messages. Defaults to "False"
  # NLP_DEBUG_IS_INFO: "False"

  ### service-nlp-matcher env variables ###
  
  # amount of items we store in the memory cache, default 20000
  # NLP_MATCHER_CACHE_ITEMS: ""

  # batch size for get keyphrase tokens
  # GET_TOKENS_BATCH_SIZE: ""

  # log level
  # NLP_MATCHER_LOG_LEVEL: ""

  # Prefetch & TTL on the read and write queues.
  # When you change this in a rolling update, you will have to scale down service matcher first, so that the existing queues are gone.
  # You cannot change queues properties of existing queues.
  # NLP_MATCHER_PREFETCH_COUNT_READ: ""
  # NLP_MATCHER_PREFETCH_COUNT_WRITE: ""
  # NLP_MATCHER_QUEUE_TTL_IN_SECS_READ: ""
  # NLP_MATCHER_QUEUE_TTL_IN_SECS_WRITE: ""

  ### service-nlp-ner env variables ###

  # Maximum restarts of child process before process exit
  # NER_MAX_CHILD_PROCESS_RESTARTS: ""

  # Timeout in ms before restarting the child process on error
  # NER_CHILD_PROCESS_RESTART_TIMEOUT: ""

  # Timeout in ms for getting the results from duckling & rustling
  # NER_CHILD_PROCESS_REPLY_TIMEOUT: ""

  # rabbitmq prefetch for getNer call
  # GET_NER_PREFETCH: ""

  # disable starting rustling & duckling http severs as child processes
  # NLP_NER_DISABLE_CHILD_PROCESSES: ""

amazonCredentials:
  ## The client id from amazon.developers.com
  clientId: ""
  ## The client secret from amazon.developers.com
  clientSecret: ""
  ## Existing secret with amazon credentials. The secret must have the following two keys:
  ##   "amazon-client-id": The client id from amazon.developers.com
  ##   "amazon-client-secret": The client secret from amazon.developers.com
  ##
  ## NOTE: When amazonCredentials.existingSecret is set, clear text credentials passed in the previous parameters
  ## "amazonCredentials.clientId" and "amazonCredentials.clientSecret" are ignored.
  existingSecret: ""

##
## Stateful Backend Components
##
statefulRabbitMq:
  image: cognigy.azurecr.io/rabbitmq:3.9.20_cognigy-4.X
  replicaCount: 1
  resources:
    limits:
      memory: 2Gi
      cpu: "2"
    requests:
      memory: 1Gi
      cpu: "1"
  extraEnvVars: []
  securityContext:
    runAsUser: 1337
    runAsGroup: 1337
  ## Optionally specify affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}
  ## Optionally specify node labels for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  ##
  nodeSelector: {}
  ## Optionally specify tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  ##
  tolerations: []

# If redis is not required to deploy then you can set the "statefulRedis.enabled" flag to false. By default it is always enabled. 
# statefulRedis:
#   enabled: false

statefulRedis:
  enabled: true
  image: cognigy.azurecr.io/redis:5.0.14_cognigy_4.X
  replicaCount: 1
  resources:
    limits:
      memory: 512Mi
      cpu: "0.5"
    requests:
      memory: 100Mi
      cpu: "0.2"
  extraEnvVars: []
  securityContext: {}
  ## Optionally specify affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}
  ## Optionally specify node labels for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  ##
  nodeSelector: {}
  ## Optionally specify tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  ##
  tolerations: []

# If redis-persistent is not required to deploy then you can set the "statefulRedis.enabled" flag to false. By default it is always enabled.
# statefulRedisPersistent: 
#   enabled: false

statefulRedisPersistent:
  enabled: true
  image: cognigy.azurecr.io/redis:5.0.14_cognigy_4.X
  replicaCount: 1
  resources:
    limits:
      memory: 512Mi
      cpu: "0.5"
    requests:
      memory: 100Mi
      cpu: "0.2"
  extraEnvVars: []
  securityContext: {}
  ## Optionally specify affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}
  ## Optionally specify node labels for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  ##
  nodeSelector: {}
  ## Optionally specify tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  ##
  tolerations: []


##
## Cognigy.AI components
##
serviceAi:
  image: cognigy.azurecr.io/service-ai:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  ## Optionally specify list of additional volumes
  ## Examples:
  ## extraVolumes:
  ##   - name: foo
  ##     secret:
  ##       secretName: mysecret
  ##       optional: false
  ##   - name: config-volume
  ##     configMap:
  ##       name: special-config
  ##       items:
  ##       - key: SPECIAL_LEVEL
  ##         path: keys
  extraVolumes: []
  ## Optionally specify list of additional volumeMounts
  ## Examples:
  ## extraVolumeMounts:
  ##   - name: foo
  ##     mountPath: "/etc/foo"
  ##     readOnly: true
  ##   - name: config-volume
  ##     mountPath: /etc/config
  extraVolumeMounts: []
  ## Optionally specify list of extra environment variables to add to the container
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  securityContext: {}
  ## Optionally enable HorizontalPodAutoscaler for pods
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  ## Make sure that pods have resources limits and requests defined and the 'metrics server' has been deployed and configured in the cluster.
  ## https://github.com/kubernetes-sigs/metrics-server
  ##
  ##
  ## Note: This is a experimental feature and should not be used in production. Please don't enable this flag for now if you are a
  ## customer running Cognigy.AI/Cognigy Insights using this HelmChart. This note will be removed once we are done with the
  ## testing and the feature will be production ready.
  horizontalPodAutoscaler:
    ## Whether enable horizontal pod autoscaler
    enabled: false
    ## Define the minimum allowed replicas to which the scaling target can be scaled down
    minReplicas: 3
    ## Define the maximum allowed replicas to which the scaling target can be scaled up
    maxReplicas: 10
    ## Define metrics against which HorizontalPodAutoscaler will react
    ## metrics:
    ##   - type: Resource
    ##     resource:
    ##       name: memory
    ##       target:
    ##         type: Utilization
    ##         averageUtilization: 80
    ##   - type: Resource
    ##     resource:
    ##       name: cpu
    ##       target:
    ##         type: Utilization
    ##         averageUtilization: 70
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            ## Define the CPU target to trigger the scaling actions (utilization percentage)
            averageUtilization: 70

serviceAlexaManagement:
  image: cognigy.azurecr.io/service-alexa-management:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceAnalyticsCollector:
  image: cognigy.azurecr.io/service-analytics-collector:release-1d148f1-1667234246
  replicaCount: 3
  resources:
    requests:
      cpu: '0.300'
      memory: 160M
    limits:
      cpu: '0.300'
      memory: 200M
  extraEnvVars: []
  securityContext: {}
serviceAnalyticsConversations:
  image: cognigy.azurecr.io/service-analytics-conversations:release-e34c9f0-1667233016
  replicaCount: 3
  requests:
    cpu: '0.1'
    memory: 120M
  limits:
    cpu: '0.3'
    memory: 250M
  extraEnvVars: []
  securityContext: {}
serviceAnalyticsOdata:
  image: cognigy.azurecr.io/service-analytics-odata:release-512970b-1666605914
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 360M
    limits:
      cpu: '0.5'
      memory: 450M
  extraEnvVars: []
  securityContext: {}
serviceAnalyticsReporter:
  image: cognigy.azurecr.io/service-analytics-reporter:release-a206c2a-1667232992
  replicaCount: 3
  resources:
    requests:
      cpu: '0.5'
      memory: 500M
    limits:
      cpu: '0.5'
      memory: 750M
  extraEnvVars: []
  securityContext: {}
serviceApi:
  image: cognigy.azurecr.io/service-api:a34c4f1e188583c141f56c10a81c86f06cbb8ff1
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 280M
    limits:
      cpu: '0.4'
      memory: 350M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
serviceAppSessionManager:
  image: cognigy.azurecr.io/service-app-session-manager:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
serviceCustomModules:
  image: cognigy.azurecr.io/service-custom-modules:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.3'
      memory: 512M
    limits:
      cpu: '0.3'
      memory: 512M
  extraEnvVars: []
  securityContext: {}
serviceEndpoint:
  image: cognigy.azurecr.io/service-endpoint:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 120M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceExecution:
  image: cognigy.azurecr.io/service-execution:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '1'
      memory: 240M
    limits:
      cpu: '1'
      memory: 300M
  extraEnvVars: []
  securityContext: {}
serviceFunctionExecution:
  image: cognigy.azurecr.io/service-function-execution:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '1'
      memory: 512M
    limits:
      cpu: '2'
      memory: 512M
  extraEnvVars: []
  securityContext: {}
serviceFunctionScheduler:
  image: cognigy.azurecr.io/service-function-scheduler:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceHandover:
  image: cognigy.azurecr.io/service-handover:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceHttp:
  image: cognigy.azurecr.io/service-http:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.1'
      memory: 75M
  extraEnvVars: []
  securityContext: {}
serviceInsightsApi:
  image: cognigy.azurecr.io/service-insights-api:release-f9e6bab-1666961505
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.1'
      memory: 75M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
serviceInsightsUi:
  image: cognigy.azurecr.io/service-insights-ui:release-ef2d283-1666605919
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
serviceJourneys:
  image: cognigy.azurecr.io/service-journeys:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 100M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceLogs:
  image: cognigy.azurecr.io/service-logs:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 100M
    limits:
      cpu: '0.5'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceNlpMatcher:
  image: cognigy.azurecr.io/service-nlp-matcher:f984f883efac025867a1203b868cf6c32e8e92b0
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 300M
    limits:
      cpu: '0.5'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
serviceNlpNer:
  image: cognigy.azurecr.io/service-nlp-ner:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.3'
      memory: 100M
    limits:
      cpu: '1.0'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceParser:
  image: cognigy.azurecr.io/service-parser:344ba692381b7096ae1ca688f1ea6d175b002b81
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
servicePlaybookExecution:
  image: cognigy.azurecr.io/service-playbook-execution:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceProfiles:
  image: cognigy.azurecr.io/service-profiles:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceResources:
  image: cognigy.azurecr.io/service-resources:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 512M
    limits:
      cpu: '0.5'
      memory: 512M
  extraEnvVars: []
  securityContext: {}
serviceRuntimeFileManager:
  image: cognigy.azurecr.io/service-runtime-file-manager:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
serviceSecurity:
  image: cognigy.azurecr.io/service-security:4ded3f0ff82a245c75a22eeb2b5a2f197c5b6066
  replicaCount: 3
  resources:
    requests:
      cpu: '0.2'
      memory: 60M
    limits:
      cpu: '0.4'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceSessionStateManager:
  image: cognigy.azurecr.io/service-session-state-manager:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
serviceStaticFiles:
  image: cognigy.azurecr.io/service-static-files:29b576edb4a4bd289a7de331df17e534a73efab8
  replicaCount: 3
  resources:
    requests:
      cpu: '0.4'
      memory: 400M
    limits:
      cpu: '0.4'
      memory: 500M
  extraEnvVars: []
  securityContext: {}
serviceTaskManager:
  image: cognigy.azurecr.io/service-task-manager:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceTrainer:
  image: cognigy.azurecr.io/service-trainer:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceUi:
  image: cognigy.azurecr.io/service-ui:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraVolumes: []
  extraVolumeMounts: []
  extraEnvVars: []
  securityContext: {}
serviceWebchat:
  image: cognigy.azurecr.io/service-webchat:67bf959d2591bc04a9d856518e7bf0b8d94095fb
  replicaCount: 3
  resources:
    requests:
      cpu: '0.1'
      memory: 60M
    limits:
      cpu: '0.3'
      memory: 150M
  extraEnvVars: []
  securityContext: {}
serviceNlpQaDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-qa-de:ddb2c18f313651a84a5d13f41f854af187eb4698
  replicaCount: 1
  resources:
    requests:
      cpu: '1'
      memory: 3G
    limits:
      cpu: '1'
      memory: 3G
  extraEnvVars: []
  securityContext: {}
serviceNlpScoreDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpTrainDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpQaEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-qa-en:ddb2c18f313651a84a5d13f41f854af187eb4698
  replicaCount: 1
  resources:
    requests:
      cpu: '1'
      memory: 3G
    limits:
      cpu: '1'
      memory: 3G
  extraEnvVars: []
  securityContext: {}
serviceNlpScoreEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-en:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpTrainEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-en:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpQaGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-qa-ge:ddb2c18f313651a84a5d13f41f854af187eb4698
  replicaCount: 1
  resources:
    requests:
      cpu: '1'
      memory: 3G
    limits:
      cpu: '1'
      memory: 3G
  extraEnvVars: []
  securityContext: {}
serviceNlpScoreGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ge:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 3000M
    limits:
      cpu: '2'
      memory: 4000M
  extraEnvVars: []
  securityContext: {}
serviceNlpTrainGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ge:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 3000M
    limits:
      cpu: '2'
      memory: 4000M
  extraEnvVars: []
  securityContext: {}
serviceNlpScoreJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ja:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpTrainJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ja:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpScoreKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ko:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpTrainKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-ko:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpScoreXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpTrainXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp:617232c7ab66d2bbe05469bfc8d8fbde242aa2fb
  replicaCount: 2
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
  extraEnvVars: []
  securityContext: {}
serviceNlpEmbeddingEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-embedding-en:e0584e4af88a88ea3136cb9be0340285484c852f
  replicaCount: 1
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      cpu: 2
      memory: 3Gi
serviceNlpEmbeddingXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-embedding-xx:e0584e4af88a88ea3136cb9be0340285484c852f
  replicaCount: 1
  resources:
    requests:
      cpu: 1
      memory: 1.2Gi
    limits:
      cpu: 2
      memory: 2Gi
serviceNlpEmbeddingGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-embedding-ge:e0584e4af88a88ea3136cb9be0340285484c852f
  replicaCount: 1
  resources:
    requests:
      cpu: 1
      memory: 3Gi
    limits:
      cpu: 2
      memory: 4Gi
serviceNlpClassifierScoreEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-en:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpClassifierTrainEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-en:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpV2ScoreEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2-en:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpV2TrainEn:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2-en:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpClassifierScoreGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ge:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpClassifierTrainGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ge:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpV2ScoreGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2-ge:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 3000M
    limits:
      cpu: '2'
      memory: 4000M
serviceNlpV2TrainGe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2-ge:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 3000M
    limits:
      cpu: '2'
      memory: 4000M
serviceNlpClassifierScoreDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpClassifierTrainDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpV2ScoreDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpV2TrainDe:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpClassifierScoreJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ja:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpClassifierTrainJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ja:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpV2ScoreJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2-ja:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpV2TrainJa:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2-ja:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpClassifierScoreKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ko:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpClassifierTrainKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier-ko:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpV2ScoreKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2-ko:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpV2TrainKo:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2-ko:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpClassifierScoreXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpClassifierTrainXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-classifier:fcf436b936282008b99330b9a3776a8965bb7ccb
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 960M
    limits:
      cpu: '1.0'
      memory: 960M
serviceNlpV2ScoreXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M
serviceNlpV2TrainXx:
  enabled: false
  image: cognigy.azurecr.io/service-nlp-v2:5a0c75a5bb43ae5c3b4948ffc72bcaf46bf86ecd
  replicaCount: 1
  resources:
    requests:
      cpu: '0.350'
      memory: 800M
    limits:
      cpu: '1'
      memory: 2500M

## You can enable the pod monitor if any prometheus instance is ruinning on your cluster.
podMonitors:
  enabled: false
  ## The namespace for the pod-monitor should be the same namespace where your prometheus instance is running
  namespace: ""

# Anti Virus Scanning
clamd:
  # 4000M is the maximum
  streamMaxLength: 4000M
  maxScanSize: 300M
  maxFileSize: 100M
  tcpAddr: 0.0.0.0
  image: cognigy.azurecr.io/clamav:0.105
  replicaCount: 1
  resources:
    limits:
      cpu: 1000m
      memory: 4000Mi
    requests:
      cpu: 800m
      memory: 3000Mi

# The values below are used for Traefik Helm chart. For more information, see
# https://github.com/traefik/traefik-helm-chart
traefik:
  enabled: true
  fullnameOverride: traefik
  image:
    name: cognigy.azurecr.io/traefik
    tag: "2.6.3"
    pullPolicy: IfNotPresent
  deployment: 
    ## Specify imagePullSecrets to pull the image from private repository.
    ## Based on the information provided in "imageCredentials" parameter previously, this should be
    ## either "cognigy-registry-token" or predefined secrets.
    ## NOTE: Can be ignored if traefik is not enabled.
    imagePullSecrets:
      - name: cognigy-registry-token
    replicas: 3
  logs:
    general:
      level: INFO
    access:
      enabled: true
      filters: {}
      fields:
        general:
          defaultmode: keep
          names: {}
        headers:
          defaultmode: drop
          names: {}
  ingressClass:
    enabled: true
    isDefaultClass: true
    fallbackApiVersion: ""
  globalArguments: []
  additionalArguments:
    - "--api.insecure=true"
    - "--entryPoints.web.forwardedHeaders.insecure"
    - "--entryPoints.websecure.forwardedHeaders.insecure"
    - "--entryPoints.web.proxyProtocol.insecure"
    - "--entryPoints.websecure.proxyProtocol.insecure"
  ports:
    traefik:
      port: 9000
      expose: false
      exposedPort: 9000
      protocol: TCP
    web:
      port: 8000
      expose: true
      exposedPort: 80
      protocol: TCP
      ## NOTE: If traefik is enabled ("traefik.enabled: true"), and you provide "tls.enable: false", then the auto redirection of http to https
      ## also must be disabled by setting traefik.ports.web.redirectTo: null
      redirectTo: websecure
    websecure:
      port: 8443
      expose: true
      exposedPort: 443
      protocol: TCP
      tls:
        enabled: true
        options: ""
        certResolver: ""
        domains: []
    metrics:
      port: 9100
      expose: false
      exposedPort: 9100
      protocol: TCP
  service:
    enabled: true
    type: LoadBalancer
    annotations: {}
    annotationsTCP: {}
    annotationsUDP: {}
    labels: {}
    spec: {}
    loadBalancerSourceRanges: []
    externalIPs: []
  tlsOptions:
    default:
      minVersion: VersionTLS12
      cipherSuites:
        - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
        - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
        - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
## For the securityContext config of traefik, please refer to the official values.yaml file of the traefik
## This is the link for chart v10.19.4 for example
## https://github.com/traefik/traefik-helm-chart/blob/f24ac3c53579e0889b53a29f23a76d359ad54803/traefik/values.yaml#L490-L501
